{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.0.0\n"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n\u001b[1mDownloading and preparing dataset imdb_reviews/subwords8k/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to C:\\Users\\ezcare14\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0...\u001b[0m\nShuffling and writing examples to C:\\Users\\ezcare14\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incomplete77IQOM\\imdb_reviews-train.tfrecord\nShuffling and writing examples to C:\\Users\\ezcare14\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incomplete77IQOM\\imdb_reviews-test.tfrecord\nShuffling and writing examples to C:\\Users\\ezcare14\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incomplete77IQOM\\imdb_reviews-unsupervised.tfrecord\n\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\ezcare14\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    # Use the version pre-encoded with an ~8k vocabulary.\n",
    "    'imdb_reviews/subwords8k', \n",
    "    # Return the train/test datasets as a tuple.\n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
    "    as_supervised=True,\n",
    "    # Also return the `info` structure. \n",
    "    with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tfds.core.DatasetInfo(\n    name='imdb_reviews',\n    version=1.0.0,\n    description='Large Movie Review Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n    features=FeaturesDict({\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n    }),\n    total_num_examples=100000,\n    splits={\n        'test': 25000,\n        'train': 25000,\n        'unsupervised': 50000,\n    },\n    supervised_keys=('text', 'label'),\n    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n      title     = {Learning Word Vectors for Sentiment Analysis},\n      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n      month     = {June},\n      year      = {2011},\n      address   = {Portland, Oregon, USA},\n      publisher = {Association for Computational Linguistics},\n      pages     = {142--150},\n      url       = {http://www.aclweb.org/anthology/P11-1015}\n    }\"\"\",\n    redistribution_info=,\n)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Vocabulary size: 8185\n"
    }
   ],
   "source": [
    "print ('Vocabulary size: {}'.format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Encoded string is [4025, 222, 6307, 2327, 4043, 2120, 7975]\nThe original string: \"Hello TensorFlow.\"\n"
    }
   ],
   "source": [
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print ('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print ('The original string: \"{}\"'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4025 ----> Hell\n222 ----> o \n6307 ----> Ten\n2327 ----> sor\n4043 ----> Fl\n2120 ----> ow\n7975 ----> .\n"
    }
   ],
   "source": [
    "for ts in encoded_string:\n",
    "  print ('{} ----> {}'.format(ts, encoder.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Encoded text: [  62    9    4  301 4161  267  148    1 3240 1779]\nLabel: 0\n"
    }
   ],
   "source": [
    "for train_example, train_label in train_data.take(1):\n",
    "  print('Encoded text:', train_example[:10].numpy())\n",
    "  print('Label:', train_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'This is a big step down after the surprisingly enjoyable original. This sequel isn\\'t nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best thing about this series, but his wisecracking is toned down in this entry. The performances are all adequate, but this time the script lets us down. The action is merely routine and the plot is only mildly interesting, so I need lots of silly laughs in order to stay entertained during a \"Trancers\" movie. Unfortunately, the laughs are few and far between, and so, this film is watchable at best.'"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "encoder.decode(train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "padded_batch() missing 1 required positional argument: 'padded_shapes'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9a5275a8cbf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     .padded_batch(32))\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m test_batches = (\n",
      "\u001b[1;31mTypeError\u001b[0m: padded_batch() missing 1 required positional argument: 'padded_shapes'"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = (\n",
    "    train_data\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32))\n",
    "\n",
    "test_batches = (\n",
    "    test_data\n",
    "    .padded_batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593756101301",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}