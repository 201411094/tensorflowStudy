{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##전역 변수\n",
    "vocab_size = 4000           #단어의 종류\n",
    "sentence_len = 70           #한 문장의 최대 단어 숫자\n",
    "tag_size = 10               #태그의 종류\n",
    "vocab_len = 52               #한 단어의 최대 문자 숫자\n",
    "char_size = 93              #문자의 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일을 읽고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, 'r')\n",
    "    tagged_sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ') # 공백을 기준으로 속성을 구분한다.\n",
    "        splits[-1] = re.sub(r'\\n', '', splits[-1]) # 줄바꿈 표시 \\n을 제거한다.\n",
    "        #word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장한다.\n",
    "        sentence.append([splits[0], splits[-1]]) # 단어와 개체명 태깅만 기록한다.\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences=readfile(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSentences=readfile(\"valid.txt\")\n",
    "testSentences=readfile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장과 태그 값을 쪼개어 각각 다른 array 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperatearray(rawsentence):\n",
    "    sentences, ner_tags = [], [] \n",
    "    for tagged_sentence in rawsentence: # 14,041개의 문장 샘플을 1개씩 불러온다.\n",
    "        sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\n",
    "        sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n",
    "        ner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다.\n",
    "    return sentences, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence, train_tag = seperatearray(trainSentences)\n",
    "valid_sentence, valid_tag = seperatearray(validSentences)\n",
    "test_sentence, test_tag = seperatearray(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n ['Peter', 'Blackburn'],\n ['BRUSSELS', '1996-08-22']]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장은 문장대로(sentence_len) 단어는 단어대로(vocab_len) padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencetochar(sentences):\n",
    "    charpaddedsentence=[]\n",
    "    for sentence in sentences:\n",
    "        newSentence=[]\n",
    "        makesentence =[]\n",
    "        makesentence.extend(sentence)\n",
    "        while len(makesentence)<sentence_len:        #문장 padding\n",
    "            makesentence.append(\"#\")\n",
    "        if len(makesentence)>sentence_len:           #문장 MAX 길이에 따라 자름\n",
    "            makesentence=makesentence[:sentence_len]\n",
    "        for words in makesentence:              \n",
    "            if len(words) > vocab_len:          #단어 padding\n",
    "                words=words[:vocab_len]\n",
    "            newSentence.append([words.ljust(vocab_len,'#')])    #단어 MAX길이에 따라 자름\n",
    "            \n",
    "\n",
    "        charpaddedsentence.append(newSentence)\n",
    "    return charpaddedsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char= sentencetochar(train_sentence)\n",
    "\n",
    "valid_char = sentencetochar(valid_sentence)\n",
    "test_char = sentencetochar(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char2Idx 딕셔너리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index={\"#\":0,\"UNKNOWN\":1}\n",
    "for c in \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char_to_index[c]=len(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_size = len(char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char단위로 쪼개며 char2Idx딕셔너리 대로 int 값으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars=[c for c in data[0]]            # Character 분리\n",
    "            Sentences[i][j]=chars \n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char = addCharInformation(train_char)\n",
    "valid_char = addCharInformation(valid_char)\n",
    "test_char = addCharInformation(test_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation2(Sentences):\n",
    "    total=[]\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sen=[]\n",
    "        for j,data in enumerate(sentence):\n",
    "            changeInt=[]\n",
    "            for k, chars in enumerate(data):\n",
    "                changeInt.append(char_to_index[chars])\n",
    "            Sen.append(changeInt) # 단어, Chracter, NER을 리스트로\n",
    "        total.append(Sen)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_char_train = addCharInformation2(train_char)\n",
    "X_char_valid = addCharInformation2(valid_char)\n",
    "X_char_test = addCharInformation2(test_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.array 형변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_train = np.array([np.array(x1) for x1 in X_char_train])\n",
    "X_char_valid = np.array([np.array(x1) for x1 in X_char_valid])\n",
    "X_char_test = np.array([np.array(x1) for x1 in X_char_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14041, 70, 52)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "X_char_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 및 태그 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4000        #문장 데이터에 있는 모든 단어를 사용하지 않고 높은 빈도수를 가진 상위 약 4,000개의 단어만을 사용\n",
    "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')         #tokenizer 객체 생성\n",
    "src_tokenizer.fit_on_texts(train_sentence)                              #인덱스 구축 \n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras_preprocessing.text.Tokenizer at 0x251d6d05848>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tar_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "단어 집합의 크기 : 4000\n개체명 태깅 정보 집합의 크기 : 10\n"
    }
   ],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(train_sentence)\n",
    "y_train = tar_tokenizer.texts_to_sequences(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22']]\n"
    }
   ],
   "source": [
    "print(train_sentence[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = src_tokenizer.texts_to_sequences(valid_sentence)\n",
    "y_valid = tar_tokenizer.texts_to_sequences(valid_tag)\n",
    "\n",
    "X_test = src_tokenizer.texts_to_sequences(test_sentence)\n",
    "y_test = tar_tokenizer.texts_to_sequences(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[989, 1, 205, 629, 7, 3939, 216, 1, 3], [774, 1872], [726, 150]]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = src_tokenizer.index_word\n",
    "index_to_ner = tar_tokenizer.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'o',\n 2: 'b-loc',\n 3: 'b-per',\n 4: 'b-org',\n 5: 'i-per',\n 6: 'i-org',\n 7: 'b-misc',\n 8: 'i-loc',\n 9: 'i-misc'}"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "index_to_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "기존 문장 : ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n빈도수가 낮은 단어가 OOV 처리된 문장 : ['soccer', '-', 'japan', 'get', 'OOV', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.']\n"
    }
   ],
   "source": [
    "decoded = []\n",
    "for index in X_test[0] : # 첫번째 샘플 안의 인덱스들에 대해서\n",
    "    decoded.append(index_to_word[index]) # 다시 단어로 변환\n",
    "\n",
    "print('기존 문장 : {}'.format(test_sentence[0]))\n",
    "print('빈도수가 낮은 단어가 OOV 처리된 문장 : {}'.format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=sentence_len)\n",
    "# X_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=sentence_len)\n",
    "# y_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자0으로 채움.\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=sentence_len)\n",
    "y_valid = pad_sequences(y_valid, padding='post', maxlen=sentence_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=sentence_len)\n",
    "y_test = pad_sequences(y_test, padding='post', maxlen=sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_valid = to_categorical(y_valid, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)           ##원핫 인코딩 시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14041, 70)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================<< 모델 생성 >>================\n",
    "# 모델 1. Character CNN 추가한 functional API\n",
    "- CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = tf.keras.layers.Input(shape=(None, ),dtype='int32', name='modelInput')\n",
    "words = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'char_input:0' shape=(None, None, 52) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "character_input=tf.keras.layers.Input(shape=(None,vocab_len,),name='char_input')\n",
    "character_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_input=tf.keras.layers.Input(shape=(None,vocab_len,),name='char_input')\n",
    "embed_char_out=tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= tf.keras.layers.Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(vocab_len))(conv1d_out)\n",
    "char = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(maxpool_out)\n",
    "char = tf.keras.layers.Dropout(0.5)(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.concatenate([words, char])\n",
    "output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')(output)\n",
    "output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[words_input, character_input], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam',metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = model.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "# print(model.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =======================================================================\n",
    "# 모델 2. Subclassing Model \n",
    "- CNN 1D \n",
    "- Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharConv1D = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharMaxpool = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(52))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.CharDropout2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv1D(chrc)\n",
    "        chrc = self.CharMaxpool(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "        chrc = self.CharDropout2(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        # x= tf.concat([wrd,chrc],axis=-1)\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSub = TestModel(vocab_size,tag_size)\n",
    "# words_input = tf.keras.layers.Input(shape=(None, ),dtype='int32', name='wordInput')\n",
    "# character_input=tf.keras.layers.Input(shape=(None,52,),name='char_input')             ###위에 선언 되어있음! \n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "modelSub([words_input,character_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 128)         512000    \n_________________________________________________________________\nchar_embedding (TimeDistribu (None, None, 52, 30)      2790      \n_________________________________________________________________\ntime_distributed (TimeDistri (None, None, 52, 30)      2730      \n_________________________________________________________________\ndropout (Dropout)            (None, None, 52, 30)      0         \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, None, 1, 30)       0         \n_________________________________________________________________\ntime_distributed_2 (TimeDist (None, None, 30)          0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, None, 30)          0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_3 (TimeDist (None, None, 10)          5130      \n=================================================================\nTotal params: 1,372,570\nTrainable params: 1,372,570\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# modelSub(words_input,character_input)\n",
    "modelSub.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 22s 2ms/sample - loss: 0.1293 - precision: 0.8942 - recall: 0.7880 - val_loss: 0.0917 - val_precision: 0.9697 - val_recall: 0.8031\nEpoch 2/8\n14041/14041 [==============================] - 10s 680us/sample - loss: 0.0556 - precision: 0.9574 - recall: 0.8857 - val_loss: 0.0571 - val_precision: 0.9531 - val_recall: 0.8997\nEpoch 3/8\n14041/14041 [==============================] - 10s 682us/sample - loss: 0.0377 - precision: 0.9620 - recall: 0.9326 - val_loss: 0.0481 - val_precision: 0.9587 - val_recall: 0.9196\nEpoch 4/8\n14041/14041 [==============================] - 10s 678us/sample - loss: 0.0281 - precision: 0.9698 - recall: 0.9508 - val_loss: 0.0380 - val_precision: 0.9684 - val_recall: 0.9379\nEpoch 5/8\n14041/14041 [==============================] - 10s 680us/sample - loss: 0.0216 - precision: 0.9764 - recall: 0.9622 - val_loss: 0.0336 - val_precision: 0.9694 - val_recall: 0.9482\nEpoch 6/8\n14041/14041 [==============================] - 10s 682us/sample - loss: 0.0184 - precision: 0.9793 - recall: 0.9675 - val_loss: 0.0299 - val_precision: 0.9755 - val_recall: 0.9523\nEpoch 7/8\n14041/14041 [==============================] - 10s 680us/sample - loss: 0.0158 - precision: 0.9822 - recall: 0.9722 - val_loss: 0.0285 - val_precision: 0.9743 - val_recall: 0.9558\nEpoch 8/8\n14041/14041 [==============================] - 10s 683us/sample - loss: 0.0127 - precision: 0.9858 - recall: 0.9774 - val_loss: 0.0248 - val_precision: 0.9777 - val_recall: 0.9622\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2548ab56cc8>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "modelSub.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9540\n\n 테스트 정확도 : 0.9690\n[0.030983985403578003, 0.9651884, 0.9431203]\n"
    }
   ],
   "source": [
    "loss,pre,recall = modelSub.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "print(modelSub.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================================================\n",
    "# 모델 3. CNN을 추가한 모델\n",
    "- Conv 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel2D (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharEmbedding2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharConv2D = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(kernel_size=3, filters=6, padding='same',activation='tanh', strides=1))\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharMaxpool = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D([52,30]))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.CharDropout2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharEmbedding2(chrc)\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        chrc = self.CharMaxpool(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "        chrc = self.CharDropout2(chrc)\n",
    "        # x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x= tf.concat([wrd,chrc],axis=-1)\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSub2D = TestModel2D(vocab_size,tag_size)\n",
    "modelSub2D([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelSub2D.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "modelSub2D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelSub2D.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = modelSub2D.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "print(modelSub2D.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# 모델 4.\n",
    "## TimeDistributed layer 빼기\n",
    "(feat. 간략화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel3 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv1D =tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, input_shape=(70,52))\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharDropout1(inp[1])\n",
    "        chrc = self.CharConv1D(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3 = TestModel3(vocab_size,tag_size)\n",
    "model3([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = model3.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "# print(model3.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 5. Conv2D\n",
    "### Embedding 사용하여 차원 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel5 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=sentence_len, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5), mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv2D =tf.keras.layers.Conv2D(kernel_size=(3,3), filters=30, padding='same',activation='tanh', strides=1, input_shape=(70,52,30))\n",
    "        self.CharMaxpooling =  tf.keras.layers.MaxPooling2D(pool_size=(1,52))\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        chrc = self.CharMaxpooling(chrc)\n",
    "        if wrd.shape[1]!=None:\n",
    "            reshaper = tf.keras.layers.Reshape((wrd.shape[1], 30))\n",
    "        else:\n",
    "            reshaper = tf.keras.layers.Reshape((-1, 30))\n",
    "        chrc = reshaper(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5 = TestModel5(vocab_size,tag_size)\n",
    "aaa =model5([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = model5.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================================================\n",
    "# 모델 6. SeparableConv1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel6 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=sentence_len, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5), mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv2D =tf.keras.layers.SeparableConv2D(kernel_size=(1,52), filters=30,activation='tanh', strides=1, input_shape=(70,52,30))\n",
    "        # self.CharMaxpooling =  tf.keras.layers.MaxPooling2D(pool_size=(1, 52))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        # chrc = self.CharMaxpooling(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "  \n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = TestModel6(vocab_size,tag_size)\n",
    "model6([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model6.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model6.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = model6.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596521709101",
   "display_name": "Python 3.7.7 64-bit ('shyun': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}