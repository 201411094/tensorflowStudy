{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##전역 변수\n",
    "vocab_size = 4000           #단어의 종류\n",
    "sentence_len = 70           #한 문장의 최대 단어 숫자\n",
    "tag_size = 10               #태그의 종류\n",
    "vocab_len = 52               #한 단어의 최대 문자 숫자\n",
    "char_size = 93              #문자의 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일을 읽고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, 'r')\n",
    "    tagged_sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ') # 공백을 기준으로 속성을 구분한다.\n",
    "        splits[-1] = re.sub(r'\\n', '', splits[-1]) # 줄바꿈 표시 \\n을 제거한다.\n",
    "        #word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장한다.\n",
    "        sentence.append([splits[0], splits[-1]]) # 단어와 개체명 태깅만 기록한다.\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences=readfile(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSentences=readfile(\"valid.txt\")\n",
    "testSentences=readfile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장과 태그 값을 쪼개어 각각 다른 array 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperatearray(rawsentence):\n",
    "    sentences, ner_tags = [], [] \n",
    "    for tagged_sentence in rawsentence: # 14,041개의 문장 샘플을 1개씩 불러온다.\n",
    "        sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\n",
    "        sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n",
    "        ner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다.\n",
    "    return sentences, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence, train_tag = seperatearray(trainSentences)\n",
    "valid_sentence, valid_tag = seperatearray(validSentences)\n",
    "test_sentence, test_tag = seperatearray(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n ['Peter', 'Blackburn'],\n ['BRUSSELS', '1996-08-22']]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장은 문장대로(sentence_len) 단어는 단어대로(vocab_len) padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencetochar(sentences):\n",
    "    charpaddedsentence=[]\n",
    "    for sentence in sentences:\n",
    "        newSentence=[]\n",
    "        makesentence =[]\n",
    "        makesentence.extend(sentence)\n",
    "        while len(makesentence)<sentence_len:        #문장 padding\n",
    "            makesentence.append(\"#\")\n",
    "        if len(makesentence)>sentence_len:           #문장 MAX 길이에 따라 자름\n",
    "            makesentence=makesentence[:sentence_len]\n",
    "        for words in makesentence:              \n",
    "            if len(words) > vocab_len:          #단어 padding\n",
    "                words=words[:vocab_len]\n",
    "            newSentence.append([words.ljust(vocab_len,'#')])    #단어 MAX길이에 따라 자름\n",
    "            \n",
    "\n",
    "        charpaddedsentence.append(newSentence)\n",
    "    return charpaddedsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char= sentencetochar(train_sentence)\n",
    "\n",
    "valid_char = sentencetochar(valid_sentence)\n",
    "test_char = sentencetochar(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char2Idx 딕셔너리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index={\"#\":0,\"UNKNOWN\":1}\n",
    "for c in \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char_to_index[c]=len(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_size = len(char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char단위로 쪼개며 char2Idx딕셔너리 대로 int 값으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars=[c for c in data[0]]            # Character 분리\n",
    "            Sentences[i][j]=chars \n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char = addCharInformation(train_char)\n",
    "valid_char = addCharInformation(valid_char)\n",
    "test_char = addCharInformation(test_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation2(Sentences):\n",
    "    total=[]\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sen=[]\n",
    "        for j,data in enumerate(sentence):\n",
    "            changeInt=[]\n",
    "            for k, chars in enumerate(data):\n",
    "                changeInt.append(char_to_index[chars])\n",
    "            Sen.append(changeInt) # 단어, Chracter, NER을 리스트로\n",
    "        total.append(Sen)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_char_train = addCharInformation2(train_char)\n",
    "X_char_valid = addCharInformation2(valid_char)\n",
    "X_char_test = addCharInformation2(test_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.array 형변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_train = np.array([np.array(x1) for x1 in X_char_train])\n",
    "X_char_valid = np.array([np.array(x1) for x1 in X_char_valid])\n",
    "X_char_test = np.array([np.array(x1) for x1 in X_char_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14041, 70, 52)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "X_char_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 및 태그 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4000        #문장 데이터에 있는 모든 단어를 사용하지 않고 높은 빈도수를 가진 상위 약 4,000개의 단어만을 사용\n",
    "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')         #tokenizer 객체 생성\n",
    "src_tokenizer.fit_on_texts(train_sentence)                              #인덱스 구축 \n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras_preprocessing.text.Tokenizer at 0x1f800095d88>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tar_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "단어 집합의 크기 : 4000\n개체명 태깅 정보 집합의 크기 : 10\n"
    }
   ],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(train_sentence)\n",
    "y_train = tar_tokenizer.texts_to_sequences(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22']]\n"
    }
   ],
   "source": [
    "print(train_sentence[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = src_tokenizer.texts_to_sequences(valid_sentence)\n",
    "y_valid = tar_tokenizer.texts_to_sequences(valid_tag)\n",
    "\n",
    "X_test = src_tokenizer.texts_to_sequences(test_sentence)\n",
    "y_test = tar_tokenizer.texts_to_sequences(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[989, 1, 205, 629, 7, 3939, 216, 1, 3], [774, 1872], [726, 150]]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = src_tokenizer.index_word\n",
    "index_to_ner = tar_tokenizer.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'o',\n 2: 'b-loc',\n 3: 'b-per',\n 4: 'b-org',\n 5: 'i-per',\n 6: 'i-org',\n 7: 'b-misc',\n 8: 'i-loc',\n 9: 'i-misc'}"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "index_to_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "기존 문장 : ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n빈도수가 낮은 단어가 OOV 처리된 문장 : ['soccer', '-', 'japan', 'get', 'OOV', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.']\n"
    }
   ],
   "source": [
    "decoded = []\n",
    "for index in X_test[0] : # 첫번째 샘플 안의 인덱스들에 대해서\n",
    "    decoded.append(index_to_word[index]) # 다시 단어로 변환\n",
    "\n",
    "print('기존 문장 : {}'.format(test_sentence[0]))\n",
    "print('빈도수가 낮은 단어가 OOV 처리된 문장 : {}'.format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=sentence_len)\n",
    "# X_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=sentence_len)\n",
    "# y_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자0으로 채움.\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=sentence_len)\n",
    "y_valid = pad_sequences(y_valid, padding='post', maxlen=sentence_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=sentence_len)\n",
    "y_test = pad_sequences(y_test, padding='post', maxlen=sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_valid = to_categorical(y_valid, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)           ##원핫 인코딩 시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14041, 70)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================<< 모델 생성 >>================\n",
    "# 모델 1. Character CNN 추가한 functional API\n",
    "- CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = tf.keras.layers.Input(shape=(None, ),dtype='int32', name='modelInput')\n",
    "words = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'char_input:0' shape=(None, None, 52) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "character_input=tf.keras.layers.Input(shape=(None,vocab_len,),name='char_input')\n",
    "character_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_input=tf.keras.layers.Input(shape=(None,vocab_len,),name='char_input')\n",
    "embed_char_out=tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= tf.keras.layers.Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(vocab_len))(conv1d_out)\n",
    "char = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(maxpool_out)\n",
    "char = tf.keras.layers.Dropout(0.5)(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([None, None, 30])"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.concatenate([words, char])\n",
    "output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')(output)\n",
    "output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nchar_input (InputLayer)         [(None, None, 52)]   0                                            \n__________________________________________________________________________________________________\nchar_embedding (TimeDistributed (None, None, 52, 30) 2790        char_input[0][0]                 \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, None, 52, 30) 0           char_embedding[0][0]             \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, None, 52, 30) 2730        dropout[0][0]                    \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, None, 1, 30)  0           time_distributed[0][0]           \n__________________________________________________________________________________________________\nmodelInput (InputLayer)         [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntime_distributed_2 (TimeDistrib (None, None, 30)     0           time_distributed_1[0][0]         \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 128)    512000      modelInput[0][0]                 \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, None, 30)     0           time_distributed_2[0][0]         \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, None, 158)    0           embedding[0][0]                  \n                                                                 dropout_1[0][0]                  \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, None, 512)    849920      concatenate[0][0]                \n__________________________________________________________________________________________________\ntime_distributed_3 (TimeDistrib (None, None, 10)     5130        bidirectional[0][0]              \n==================================================================================================\nTotal params: 1,372,570\nTrainable params: 1,372,570\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[words_input, character_input], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam',metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 21s 1ms/sample - loss: 0.1258 - precision: 0.8971 - recall: 0.7929 - val_loss: 0.0888 - val_precision: 0.9379 - val_recall: 0.8319\nEpoch 2/8\n14041/14041 [==============================] - 10s 683us/sample - loss: 0.0542 - precision: 0.9585 - recall: 0.8882 - val_loss: 0.0524 - val_precision: 0.9614 - val_recall: 0.9065\nEpoch 3/8\n14041/14041 [==============================] - 10s 679us/sample - loss: 0.0328 - precision: 0.9693 - recall: 0.9397 - val_loss: 0.0343 - val_precision: 0.9730 - val_recall: 0.9447\nEpoch 4/8\n14041/14041 [==============================] - 10s 680us/sample - loss: 0.0251 - precision: 0.9738 - recall: 0.9557 - val_loss: 0.0325 - val_precision: 0.9754 - val_recall: 0.9472\nEpoch 5/8\n14041/14041 [==============================] - 10s 682us/sample - loss: 0.0210 - precision: 0.9770 - recall: 0.9627 - val_loss: 0.0310 - val_precision: 0.9707 - val_recall: 0.9521\nEpoch 6/8\n14041/14041 [==============================] - 10s 681us/sample - loss: 0.0182 - precision: 0.9797 - recall: 0.9676 - val_loss: 0.0258 - val_precision: 0.9775 - val_recall: 0.9593\nEpoch 7/8\n14041/14041 [==============================] - 10s 683us/sample - loss: 0.0147 - precision: 0.9837 - recall: 0.9735 - val_loss: 0.0248 - val_precision: 0.9785 - val_recall: 0.9612\nEpoch 8/8\n14041/14041 [==============================] - 10s 683us/sample - loss: 0.0128 - precision: 0.9859 - recall: 0.9772 - val_loss: 0.0243 - val_precision: 0.9784 - val_recall: 0.9619\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1af86a00708>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "model.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9541\n\n 테스트 정확도 : 0.9696\n"
    }
   ],
   "source": [
    "loss,pre,recall = model.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "# print(model.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =======================================================================\n",
    "# 모델 2. Subclassing Model \n",
    "- CNN 1D \n",
    "- Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharConv1D = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharMaxpool = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(52))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.CharDropout2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv1D(chrc)\n",
    "        chrc = self.CharMaxpool(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "        chrc = self.CharDropout2(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        # x= tf.concat([wrd,chrc],axis=-1)\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSub = TestModel(vocab_size,tag_size)\n",
    "# words_input = tf.keras.layers.Input(shape=(None, ),dtype='int32', name='wordInput')\n",
    "# character_input=tf.keras.layers.Input(shape=(None,52,),name='char_input')             ###위에 선언 되어있음! \n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "modelSub([words_input,character_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, None, 128)         512000    \n_________________________________________________________________\nchar_embedding (TimeDistribu (None, None, 52, 30)      2790      \n_________________________________________________________________\ntime_distributed_4 (TimeDist (None, None, 52, 30)      2730      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, None, 52, 30)      0         \n_________________________________________________________________\ntime_distributed_5 (TimeDist (None, None, 1, 30)       0         \n_________________________________________________________________\ntime_distributed_6 (TimeDist (None, None, 30)          0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, None, 30)          0         \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_7 (TimeDist (None, None, 10)          5130      \n=================================================================\nTotal params: 1,372,570\nTrainable params: 1,372,570\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# modelSub(words_input,character_input)\n",
    "modelSub.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 18s 1ms/sample - loss: 0.1282 - precision: 0.8952 - recall: 0.7900 - val_loss: 0.0891 - val_precision: 0.9492 - val_recall: 0.8250\nEpoch 2/8\n14041/14041 [==============================] - 10s 682us/sample - loss: 0.0554 - precision: 0.9581 - recall: 0.8870 - val_loss: 0.0539 - val_precision: 0.9560 - val_recall: 0.9050\nEpoch 3/8\n14041/14041 [==============================] - 10s 682us/sample - loss: 0.0315 - precision: 0.9701 - recall: 0.9424 - val_loss: 0.0326 - val_precision: 0.9752 - val_recall: 0.9468\nEpoch 4/8\n14041/14041 [==============================] - 10s 689us/sample - loss: 0.0231 - precision: 0.9766 - recall: 0.9590 - val_loss: 0.0312 - val_precision: 0.9750 - val_recall: 0.9490\nEpoch 5/8\n14041/14041 [==============================] - 10s 678us/sample - loss: 0.0196 - precision: 0.9792 - recall: 0.9655 - val_loss: 0.0268 - val_precision: 0.9782 - val_recall: 0.9559\nEpoch 6/8\n14041/14041 [==============================] - 9s 673us/sample - loss: 0.0170 - precision: 0.9818 - recall: 0.9702 - val_loss: 0.0254 - val_precision: 0.9782 - val_recall: 0.9605\nEpoch 7/8\n14041/14041 [==============================] - 9s 675us/sample - loss: 0.0148 - precision: 0.9834 - recall: 0.9738 - val_loss: 0.0272 - val_precision: 0.9769 - val_recall: 0.9562\nEpoch 8/8\n14041/14041 [==============================] - 10s 678us/sample - loss: 0.0124 - precision: 0.9863 - recall: 0.9776 - val_loss: 0.0239 - val_precision: 0.9778 - val_recall: 0.9638\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1b268fb4c08>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "modelSub.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9567\n\n 테스트 정확도 : 0.9705\n[0.029523488811127974, 0.9672619, 0.94637734]\n"
    }
   ],
   "source": [
    "loss,pre,recall = modelSub.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "print(modelSub.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================================================\n",
    "# 모델 3. CNN을 추가한 모델\n",
    "- Conv 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel2D (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharEmbedding2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharConv2D = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(kernel_size=3, filters=6, padding='same',activation='tanh', strides=1))\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharMaxpool = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D([52,30]))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.CharDropout2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharEmbedding2(chrc)\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        chrc = self.CharMaxpool(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "        chrc = self.CharDropout2(chrc)\n",
    "        # x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x= tf.concat([wrd,chrc],axis=-1)\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model2d/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "modelSub2D = TestModel2D(vocab_size,tag_size)\n",
    "modelSub2D([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model2d\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, None, 128)         512000    \n_________________________________________________________________\nchar_embedding (TimeDistribu (None, None, 52, 30)      2790      \n_________________________________________________________________\nchar_embedding (TimeDistribu (None, None, 52, 30, 30)  2790      \n_________________________________________________________________\ntime_distributed_8 (TimeDist (None, None, 52, 30, 6)   1626      \n_________________________________________________________________\ndropout_4 (Dropout)          (None, None, 52, 30, 30)  0         \n_________________________________________________________________\ntime_distributed_9 (TimeDist (None, None, 1, 1, 6)     0         \n_________________________________________________________________\ntime_distributed_10 (TimeDis (None, None, 6)           0         \n_________________________________________________________________\ndropout_5 (Dropout)          (None, None, 6)           0         \n_________________________________________________________________\nbidirectional_2 (Bidirection multiple                  800768    \n_________________________________________________________________\ntime_distributed_11 (TimeDis multiple                  5130      \n=================================================================\nTotal params: 1,325,104\nTrainable params: 1,325,104\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "modelSub2D.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "modelSub2D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\nWARNING:tensorflow:Gradients do not exist for variables ['test_model2d/char_embedding/embeddings:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['test_model2d/char_embedding/embeddings:0'] when minimizing the loss.\n14041/14041 [==============================] - 169s 12ms/sample - loss: 0.2563 - precision: 0.9541 - recall: 0.9168 - val_loss: 0.1459 - val_precision: 0.9667 - val_recall: 0.9619\nEpoch 2/8\n14041/14041 [==============================] - 164s 12ms/sample - loss: 0.1010 - precision: 0.9878 - recall: 0.9623 - val_loss: 0.0999 - val_precision: 0.9934 - val_recall: 0.9539\nEpoch 3/8\n14041/14041 [==============================] - 165s 12ms/sample - loss: 0.0747 - precision: 0.9920 - recall: 0.9673 - val_loss: 0.0785 - val_precision: 0.9878 - val_recall: 0.9693\nEpoch 4/8\n14041/14041 [==============================] - 165s 12ms/sample - loss: 0.0569 - precision: 0.9918 - recall: 0.9754 - val_loss: 0.0657 - val_precision: 0.9879 - val_recall: 0.9761\nEpoch 5/8\n14041/14041 [==============================] - 167s 12ms/sample - loss: 0.0430 - precision: 0.9923 - recall: 0.9830 - val_loss: 0.0538 - val_precision: 0.9905 - val_recall: 0.9788\nEpoch 6/8\n14041/14041 [==============================] - 239s 17ms/sample - loss: 0.0349 - precision: 0.9928 - recall: 0.9869 - val_loss: 0.0479 - val_precision: 0.9894 - val_recall: 0.9842\nEpoch 7/8\n14041/14041 [==============================] - 168s 12ms/sample - loss: 0.0298 - precision: 0.9934 - recall: 0.9892 - val_loss: 0.0443 - val_precision: 0.9903 - val_recall: 0.9852\nEpoch 8/8\n14041/14041 [==============================] - 167s 12ms/sample - loss: 0.0268 - precision: 0.9939 - recall: 0.9903 - val_loss: 0.0435 - val_precision: 0.9900 - val_recall: 0.9855\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1b280118ac8>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "modelSub2D.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9867\n\n 테스트 정확도 : 0.9537\n[0.046318683964776154, 0.98919743, 0.9842373]\n"
    }
   ],
   "source": [
    "loss,pre,recall = modelSub2D.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "print(modelSub2D.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# 모델 4.\n",
    "## TimeDistributed layer 빼기\n",
    "(feat. 간략화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel3 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv1D =tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, input_shape=(70,52))\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharDropout1(inp[1])\n",
    "        chrc = self.CharConv1D(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model3/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "model3 = TestModel3(vocab_size,tag_size)\n",
    "model3([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_7 (Embedding)      (None, None, 128)         512000    \n_________________________________________________________________\ndropout_6 (Dropout)          (None, None, 52)          0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, None, 30)          4710      \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_12 (TimeDis (None, None, 10)          5130      \n=================================================================\nTotal params: 1,371,760\nTrainable params: 1,371,760\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model3.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 12s 890us/sample - loss: 0.1301 - precision: 0.9005 - recall: 0.7762 - val_loss: 0.0784 - val_precision: 0.9496 - val_recall: 0.8545\nEpoch 2/8\n14041/14041 [==============================] - 4s 264us/sample - loss: 0.0564 - precision: 0.9593 - recall: 0.8835 - val_loss: 0.0522 - val_precision: 0.9528 - val_recall: 0.9176\nEpoch 3/8\n14041/14041 [==============================] - 4s 260us/sample - loss: 0.0364 - precision: 0.9649 - recall: 0.9332 - val_loss: 0.0434 - val_precision: 0.9578 - val_recall: 0.9331\nEpoch 4/8\n14041/14041 [==============================] - 4s 260us/sample - loss: 0.0288 - precision: 0.9690 - recall: 0.9494 - val_loss: 0.0382 - val_precision: 0.9628 - val_recall: 0.9421\nEpoch 5/8\n14041/14041 [==============================] - 4s 257us/sample - loss: 0.0243 - precision: 0.9728 - recall: 0.9580 - val_loss: 0.0377 - val_precision: 0.9651 - val_recall: 0.9387\nEpoch 6/8\n14041/14041 [==============================] - 4s 256us/sample - loss: 0.0213 - precision: 0.9756 - recall: 0.9629 - val_loss: 0.0406 - val_precision: 0.9590 - val_recall: 0.9481\nEpoch 7/8\n14041/14041 [==============================] - 4s 258us/sample - loss: 0.0190 - precision: 0.9779 - recall: 0.9671 - val_loss: 0.0359 - val_precision: 0.9648 - val_recall: 0.9472\nEpoch 8/8\n14041/14041 [==============================] - 4s 256us/sample - loss: 0.0169 - precision: 0.9803 - recall: 0.9706 - val_loss: 0.0405 - val_precision: 0.9592 - val_recall: 0.9490\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1b297059488>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "model3.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9390\n\n 테스트 정확도 : 0.9542\n"
    }
   ],
   "source": [
    "loss,pre,recall = model3.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "# print(model3.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 5. Conv2D\n",
    "### Embedding 사용하여 차원 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel5 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5), mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv2D =tf.keras.layers.Conv2D(kernel_size=(3,52), filters=30, padding='same',activation='tanh', strides=1, input_shape=(70,52,30))\n",
    "        # self.CharMaxpooling =  tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPool1D(52))\n",
    "        self.CharMaxpooling =  tf.keras.layers.MaxPooling2D(pool_size=(1,52))\n",
    "        #self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        # self.CharFlatten = tf.keras.layers.Flatten(data_format='channels_last')\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        chrc = self.CharMaxpooling(chrc)\n",
    "        # chrc = self.CharFlatten(chrc)\n",
    "        chrc = tf.squeeze(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-74-c62db5486241>:23 call  *\n        x = tf.keras.layers.concatenate([wrd,chrc])\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\merge.py:705 concatenate\n        return Concatenate(axis=axis, **kwargs)(inputs)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:305 wrapper\n        input_shape = convert_shapes(input_shape, to_tuples=True)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:231 convert_shapes\n        input_shape)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:179 map_structure_with_atomic\n        map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:179 <listcomp>\n        map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:168 map_structure_with_atomic\n        return map_fn(nested)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:227 _convert_shape\n        input_shape = tuple(input_shape.as_list())\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py:1171 as_list\n        raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-678d25d8076d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestModel5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcharacter_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# modelSub.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    841\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-74-c62db5486241>:23 call  *\n        x = tf.keras.layers.concatenate([wrd,chrc])\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\merge.py:705 concatenate\n        return Concatenate(axis=axis, **kwargs)(inputs)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:817 __call__\n        self._maybe_build(inputs)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:2141 _maybe_build\n        self.build(input_shapes)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:305 wrapper\n        input_shape = convert_shapes(input_shape, to_tuples=True)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:231 convert_shapes\n        input_shape)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:179 map_structure_with_atomic\n        map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:179 <listcomp>\n        map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:168 map_structure_with_atomic\n        return map_fn(nested)\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:227 _convert_shape\n        input_shape = tuple(input_shape.as_list())\n    C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py:1171 as_list\n        raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n"
     ]
    }
   ],
   "source": [
    "model5 = TestModel5(vocab_size,tag_size)\n",
    "model5([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model5_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_21 (Embedding)     (None, None, 128)         512000    \n_________________________________________________________________\nembedding_22 (Embedding)     (None, None, 52, 30)      2790      \n_________________________________________________________________\ndropout_10 (Dropout)         (None, None, 52, 30)      0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, None, 52, 30)      140430    \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, None, 1, 30)       0         \n_________________________________________________________________\ntime_distributed_20 (TimeDis (None, None, 30)          0         \n_________________________________________________________________\nbidirectional_10 (Bidirectio (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_21 (TimeDis (None, None, 10)          5130      \n=================================================================\nTotal params: 1,510,270\nTrainable params: 1,510,270\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model5.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 28s 2ms/sample - loss: 0.1221 - precision: 0.8980 - recall: 0.8021 - val_loss: 0.0787 - val_precision: 0.9448 - val_recall: 0.8503\nEpoch 2/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.0443 - precision: 0.9725 - recall: 0.9030 - val_loss: 0.0359 - val_precision: 0.9779 - val_recall: 0.9348\nEpoch 3/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.0224 - precision: 0.9826 - recall: 0.9560 - val_loss: 0.0257 - val_precision: 0.9816 - val_recall: 0.9567\nEpoch 4/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.0159 - precision: 0.9856 - recall: 0.9704 - val_loss: 0.0222 - val_precision: 0.9816 - val_recall: 0.9631\nEpoch 5/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.0131 - precision: 0.9872 - recall: 0.9761 - val_loss: 0.0212 - val_precision: 0.9805 - val_recall: 0.9669\nEpoch 6/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.0111 - precision: 0.9889 - recall: 0.9797 - val_loss: 0.0219 - val_precision: 0.9787 - val_recall: 0.9666\nEpoch 7/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.0094 - precision: 0.9902 - recall: 0.9832 - val_loss: 0.0207 - val_precision: 0.9812 - val_recall: 0.9697\nEpoch 8/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.0085 - precision: 0.9912 - recall: 0.9849 - val_loss: 0.0201 - val_precision: 0.9806 - val_recall: 0.9706\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1fb3efd3d48>"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "model5.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9600\n\n 테스트 정확도 : 0.9729\n"
    }
   ],
   "source": [
    "loss,pre,recall = model5.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================================================\n",
    "# 모델 6. SeparableConv1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel6 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv1D =tf.keras.layers.SeparableConv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, input_shape=(70,52))\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharDropout1(inp[1])\n",
    "        chrc = self.CharConv1D(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model6/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "model6 = TestModel6(vocab_size,tag_size)\n",
    "model6([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_27 (Embedding)     (None, None, 128)         512000    \n_________________________________________________________________\ndropout_13 (Dropout)         (None, None, 52)          0         \n_________________________________________________________________\nseparable_conv1d (SeparableC (None, None, 30)          1746      \n_________________________________________________________________\nbidirectional_13 (Bidirectio (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_24 (TimeDis (None, None, 10)          5130      \n=================================================================\nTotal params: 1,368,796\nTrainable params: 1,368,796\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model6.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n  128/14041 [..............................] - ETA: 4:42"
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2382\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2384\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m       \u001b[0mxla_compile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaCompile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2386\u001b[0m       \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2387\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2388\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-4d2c2035c951>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_char_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_char_valid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 408\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2150\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[1;34m(input_iterator)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[1;32m---> 73\u001b[1;33m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[0;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[1;32m--> 760\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[1;32m-> 2132\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m               training=training))\n\u001b[0m\u001b[0;32m    253\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m   \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-e4d736af2043>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mchrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCharConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchrc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfBiLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[0;32m    642\u001b[0m                              initial_state=forward_state, **kwargs)\n\u001b[0;32m    643\u001b[0m       y_rev = self.backward_layer(backward_inputs,\n\u001b[1;32m--> 644\u001b[1;33m                                   initial_state=backward_state, **kwargs)\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    965\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         (last_output, outputs, new_h, new_c,\n\u001b[1;32m--> 967\u001b[1;33m          runtime) = lstm_with_backend_selection(**normal_lstm_kwargs)\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m       \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mlstm_with_backend_selection\u001b[1;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, activation, recurrent_activation)\u001b[0m\n\u001b[0;32m   1287\u001b[0m   last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(\n\u001b[0;32m   1288\u001b[0m       **params)\n\u001b[1;32m-> 1289\u001b[1;33m   \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_cudnn_lstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mregister\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2176\u001b[0m   \u001b[0mconcrete_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2177\u001b[0m   \u001b[0mconcrete_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2178\u001b[1;33m   \u001b[0mconcrete_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_gradient_functions_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2179\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconcrete_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36madd_gradient_functions_to_graph\u001b[1;34m(self, g)\u001b[0m\n\u001b[0;32m   1339\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m     forward_function, backward_function = (\n\u001b[1;32m-> 1341\u001b[1;33m         self._delayed_rewrite_functions.forward_backward())\n\u001b[0m\u001b[0;32m   1342\u001b[0m     \u001b[0mforward_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[0mbackward_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m     \u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    627\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m           \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m           func_graph=backwards_graph)\n\u001b[0m\u001b[0;32m    630\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m       captures_from_forward = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[1;34m(*grad_ys)\u001b[0m\n\u001b[0;32m    617\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m           \u001b[0mgrad_ys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m           src_graph=self._func_graph)\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaScope\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exit early\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_IfGrad\u001b[1;34m(op, *grads)\u001b[0m\n\u001b[0;32m    165\u001b[0m   \u001b[1;31m# they are in-scope, i.e., belong to one of outer graphs of the grad graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m   \u001b[0mtrue_grad_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_resolve_grad_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_grad_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m   \u001b[0mfalse_grad_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_resolve_grad_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse_grad_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m   \u001b[1;31m# This modifies true_grad_graph and false_grad_graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_resolve_grad_inputs\u001b[1;34m(cond_graph, grad_graph)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;31m# `cond_graph`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgrad_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m       \u001b[1;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcond_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m       \u001b[1;31m# `internal_captures` are not treated as intermediates and hence not added\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m       \u001b[1;31m# to If op outputs. So we get the outer tensor corresponding to those\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model6.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595809135869",
   "display_name": "Python 3.7.7 64-bit ('shyun': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}