{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##전역 변수\n",
    "vocab_size = 4000           #단어의 종류\n",
    "sentence_len = 70           #한 문장의 최대 단어 숫자\n",
    "tag_size = 10               #태그의 종류\n",
    "vocab_len = 52               #한 단어의 최대 문자 숫자\n",
    "char_size = 93              #문자의 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일을 읽고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, 'r')\n",
    "    tagged_sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ') # 공백을 기준으로 속성을 구분한다.\n",
    "        splits[-1] = re.sub(r'\\n', '', splits[-1]) # 줄바꿈 표시 \\n을 제거한다.\n",
    "        #word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장한다.\n",
    "        sentence.append([splits[0], splits[-1]]) # 단어와 개체명 태깅만 기록한다.\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences=readfile(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSentences=readfile(\"valid.txt\")\n",
    "testSentences=readfile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장과 태그 값을 쪼개어 각각 다른 array 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperatearray(rawsentence):\n",
    "    sentences, ner_tags = [], [] \n",
    "    for tagged_sentence in rawsentence: # 14,041개의 문장 샘플을 1개씩 불러온다.\n",
    "        sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\n",
    "        sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n",
    "        ner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다.\n",
    "    return sentences, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence, train_tag = seperatearray(trainSentences)\n",
    "valid_sentence, valid_tag = seperatearray(validSentences)\n",
    "test_sentence, test_tag = seperatearray(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n ['Peter', 'Blackburn'],\n ['BRUSSELS', '1996-08-22']]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_sentence[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장은 문장대로(sentence_len) 단어는 단어대로(vocab_len) padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencetochar(sentences):\n",
    "    charpaddedsentence=[]\n",
    "    for sentence in sentences:\n",
    "        newSentence=[]\n",
    "        makesentence =[]\n",
    "        makesentence.extend(sentence)\n",
    "        while len(makesentence)<sentence_len:        #문장 padding\n",
    "            makesentence.append(\"#\")\n",
    "        if len(makesentence)>sentence_len:           #문장 MAX 길이에 따라 자름\n",
    "            makesentence=makesentence[:sentence_len]\n",
    "        for words in makesentence:              \n",
    "            if len(words) > vocab_len:          #단어 padding\n",
    "                words=words[:vocab_len]\n",
    "            newSentence.append([words.ljust(vocab_len,'#')])    #단어 MAX길이에 따라 자름\n",
    "            \n",
    "\n",
    "        charpaddedsentence.append(newSentence)\n",
    "    return charpaddedsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char= sentencetochar(train_sentence)\n",
    "\n",
    "valid_char = sentencetochar(valid_sentence)\n",
    "test_char = sentencetochar(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char2Idx 딕셔너리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index={\"#\":0,\"UNKNOWN\":1}\n",
    "for c in \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char_to_index[c]=len(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_size = len(char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char단위로 쪼개며 char2Idx딕셔너리 대로 int 값으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars=[c for c in data[0]]            # Character 분리\n",
    "            Sentences[i][j]=chars \n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char = addCharInformation(train_char)\n",
    "valid_char = addCharInformation(valid_char)\n",
    "test_char = addCharInformation(test_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation2(Sentences):\n",
    "    total=[]\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sen=[]\n",
    "        for j,data in enumerate(sentence):\n",
    "            changeInt=[]\n",
    "            for k, chars in enumerate(data):\n",
    "                changeInt.append(char_to_index[chars])\n",
    "            Sen.append(changeInt) # 단어, Chracter, NER을 리스트로\n",
    "        total.append(Sen)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_char_train = addCharInformation2(train_char)\n",
    "X_char_valid = addCharInformation2(valid_char)\n",
    "X_char_test = addCharInformation2(test_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.array 형변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_train = np.array([np.array(x1) for x1 in X_char_train])\n",
    "X_char_valid = np.array([np.array(x1) for x1 in X_char_valid])\n",
    "X_char_test = np.array([np.array(x1) for x1 in X_char_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14041, 70, 52)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "X_char_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 및 태그 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4000        #문장 데이터에 있는 모든 단어를 사용하지 않고 높은 빈도수를 가진 상위 약 4,000개의 단어만을 사용\n",
    "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')         #tokenizer 객체 생성\n",
    "src_tokenizer.fit_on_texts(train_sentence)                              #인덱스 구축 \n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras_preprocessing.text.Tokenizer at 0x21a81622a88>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tar_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "단어 집합의 크기 : 4000\n개체명 태깅 정보 집합의 크기 : 10\n"
    }
   ],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(train_sentence)\n",
    "y_train = tar_tokenizer.texts_to_sequences(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22']]\n"
    }
   ],
   "source": [
    "print(train_sentence[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = src_tokenizer.texts_to_sequences(valid_sentence)\n",
    "y_valid = tar_tokenizer.texts_to_sequences(valid_tag)\n",
    "\n",
    "X_test = src_tokenizer.texts_to_sequences(test_sentence)\n",
    "y_test = tar_tokenizer.texts_to_sequences(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[989, 1, 205, 629, 7, 3939, 216, 1, 3], [774, 1872], [726, 150]]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = src_tokenizer.index_word\n",
    "index_to_ner = tar_tokenizer.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'o',\n 2: 'b-loc',\n 3: 'b-per',\n 4: 'b-org',\n 5: 'i-per',\n 6: 'i-org',\n 7: 'b-misc',\n 8: 'i-loc',\n 9: 'i-misc'}"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "index_to_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "기존 문장 : ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n빈도수가 낮은 단어가 OOV 처리된 문장 : ['soccer', '-', 'japan', 'get', 'OOV', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.']\n"
    }
   ],
   "source": [
    "decoded = []\n",
    "for index in X_test[0] : # 첫번째 샘플 안의 인덱스들에 대해서\n",
    "    decoded.append(index_to_word[index]) # 다시 단어로 변환\n",
    "\n",
    "print('기존 문장 : {}'.format(test_sentence[0]))\n",
    "print('빈도수가 낮은 단어가 OOV 처리된 문장 : {}'.format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=sentence_len)\n",
    "# X_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=sentence_len)\n",
    "# y_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자0으로 채움.\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=sentence_len)\n",
    "y_valid = pad_sequences(y_valid, padding='post', maxlen=sentence_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=sentence_len)\n",
    "y_test = pad_sequences(y_test, padding='post', maxlen=sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_valid = to_categorical(y_valid, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)           ##원핫 인코딩 시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14041, 70)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================<< 모델 생성 >>================\n",
    "# 모델 1. Character CNN 추가한 functional API\n",
    "- CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = tf.keras.layers.Input(shape=(None, ),dtype='int32', name='modelInput')\n",
    "words = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'char_input:0' shape=(None, None, 52) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "character_input=tf.keras.layers.Input(shape=(None,vocab_len,),name='char_input')\n",
    "character_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_input=tf.keras.layers.Input(shape=(None,vocab_len,),name='char_input')\n",
    "embed_char_out=tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= tf.keras.layers.Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(vocab_len))(conv1d_out)\n",
    "char = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(maxpool_out)\n",
    "char = tf.keras.layers.Dropout(0.5)(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.concatenate([words, char])\n",
    "output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')(output)\n",
    "output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[words_input, character_input], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam',metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = model.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "# print(model.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =======================================================================\n",
    "# 모델 2. Subclassing Model \n",
    "- CNN 1D \n",
    "- Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharConv1D = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharMaxpool = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(52))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.CharDropout2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv1D(chrc)\n",
    "        chrc = self.CharMaxpool(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "        chrc = self.CharDropout2(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        # x= tf.concat([wrd,chrc],axis=-1)\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSub = TestModel(vocab_size,tag_size)\n",
    "# words_input = tf.keras.layers.Input(shape=(None, ),dtype='int32', name='wordInput')\n",
    "# character_input=tf.keras.layers.Input(shape=(None,52,),name='char_input')             ###위에 선언 되어있음! \n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model_1/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "modelSub([words_input,character_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_25 (Embedding)     (None, None, 128)         512000    \n_________________________________________________________________\nchar_embedding (TimeDistribu (None, None, 52, 30)      2790      \n_________________________________________________________________\ntime_distributed_26 (TimeDis (None, None, 1, 30)       46830     \n_________________________________________________________________\ndropout_13 (Dropout)         (None, None, 52, 30)      0         \n_________________________________________________________________\ntime_distributed_27 (TimeDis (None, None, 30)          0         \n_________________________________________________________________\ndropout_14 (Dropout)         (None, None, 30)          0         \n_________________________________________________________________\nbidirectional_12 (Bidirectio (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_28 (TimeDis (None, None, 10)          5130      \n=================================================================\nTotal params: 1,416,670\nTrainable params: 1,416,670\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# modelSub(words_input,character_input)\n",
    "modelSub.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 17s 1ms/sample - loss: 0.1219 - precision: 0.8991 - recall: 0.8023 - val_loss: 0.0669 - val_precision: 0.9616 - val_recall: 0.8779\nEpoch 2/8\n14041/14041 [==============================] - 8s 586us/sample - loss: 0.0418 - precision: 0.9743 - recall: 0.9097 - val_loss: 0.0346 - val_precision: 0.9760 - val_recall: 0.9451\nEpoch 3/8\n14041/14041 [==============================] - 8s 584us/sample - loss: 0.0238 - precision: 0.9804 - recall: 0.9558 - val_loss: 0.0267 - val_precision: 0.9791 - val_recall: 0.9592\nEpoch 4/8\n14041/14041 [==============================] - 8s 586us/sample - loss: 0.0177 - precision: 0.9834 - recall: 0.9683 - val_loss: 0.0236 - val_precision: 0.9806 - val_recall: 0.9637\nEpoch 5/8\n14041/14041 [==============================] - 8s 586us/sample - loss: 0.0148 - precision: 0.9859 - recall: 0.9735 - val_loss: 0.0228 - val_precision: 0.9786 - val_recall: 0.9663\nEpoch 6/8\n14041/14041 [==============================] - 8s 588us/sample - loss: 0.0126 - precision: 0.9874 - recall: 0.9775 - val_loss: 0.0221 - val_precision: 0.9795 - val_recall: 0.9664\nEpoch 7/8\n14041/14041 [==============================] - 8s 587us/sample - loss: 0.0108 - precision: 0.9892 - recall: 0.9809 - val_loss: 0.0222 - val_precision: 0.9794 - val_recall: 0.9686\nEpoch 8/8\n14041/14041 [==============================] - 8s 588us/sample - loss: 0.0094 - precision: 0.9904 - recall: 0.9837 - val_loss: 0.0224 - val_precision: 0.9788 - val_recall: 0.9696\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21e65077888>"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "modelSub.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9617\n\n 테스트 정확도 : 0.9729\n[0.027087303505584172, 0.9678018, 0.95565236]\n"
    }
   ],
   "source": [
    "loss,pre,recall = modelSub.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "print(modelSub.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================================================\n",
    "# 모델 3. CNN을 추가한 모델\n",
    "- Conv 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel2D (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharEmbedding2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')\n",
    "        self.CharConv2D = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(kernel_size=3, filters=6, padding='same',activation='tanh', strides=1))\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharMaxpool = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D([52,30]))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.CharDropout2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharEmbedding2(chrc)\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        chrc = self.CharMaxpool(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "        chrc = self.CharDropout2(chrc)\n",
    "        # x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x= tf.concat([wrd,chrc],axis=-1)\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSub2D = TestModel2D(vocab_size,tag_size)\n",
    "modelSub2D([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelSub2D.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "modelSub2D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelSub2D.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = modelSub2D.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "print(modelSub2D.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# 모델 4.\n",
    "## TimeDistributed layer 빼기\n",
    "(feat. 간략화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel3 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv1D =tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, input_shape=(70,52))\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharDropout1(inp[1])\n",
    "        chrc = self.CharConv1D(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model3_2/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "model3 = TestModel3(vocab_size,tag_size)\n",
    "model3([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model3_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_29 (Embedding)     (None, None, 128)         512000    \n_________________________________________________________________\ndropout_17 (Dropout)         (None, None, 52)          0         \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, None, 30)          4710      \n_________________________________________________________________\nbidirectional_15 (Bidirectio (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_32 (TimeDis (None, None, 10)          5130      \n=================================================================\nTotal params: 1,371,760\nTrainable params: 1,371,760\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model3.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n  128/14041 [..............................] - ETA: 13:23"
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  7 inputs specified of 11 inputs in Op while building NodeDef 'cond_65/then/_0' using Op<name=cond_true_179844_rewritten; signature=transpose_inputs:float, expanddims_init_h:float, expanddims_1_init_c:float, split_readvariableop_resource:resource, split_1_readvariableop_resource:resource, zeros_like_readvariableop_resource:resource, cast_stopgradient:bool, optionalfromvalue_60_deleteiterator_input_iterator:resource, optionalfromvalue_60_deleteiterator_input_iterator_1:variant, optionalfromvalue_60_deleteiterator_2_input_iterator:resource, optionalfromvalue_60_deleteiterator_2_input_iterator_1:variant -> identity:float, identity_1:float, identity_2:float, identity_3:float, identity_4:float, optionalfromvalue:variant, optionalfromvalue_1:variant, optionalfromvalue_2:variant, optionalfromvalue_3:variant, optionalfromvalue_4:variant, optionalfromvalue_5:variant, optionalfromvalue_6:variant, optionalfromvalue_7:variant, optionalfromvalue_8:variant, optionalfromvalue_9:variant, optionalfromvalue_10:variant, optionalfromvalue_11:variant, optionalfromvalue_12:variant, optionalfromvalue_13:variant, optionalfromvalue_14:variant, optionalfromvalue_15:variant, optionalfromvalue_16:variant, optionalfromvalue_17:variant, optionalfromvalue_18:variant, optionalfromvalue_19:variant, optionalfromvalue_20:variant, optionalfromvalue_21:variant, optionalfromvalue_22:variant, optionalfromvalue_23:variant, optionalfromvalue_24:variant, optionalfromvalue_25:variant, optionalfromvalue_26:variant, optionalfromvalue_27:variant, optionalfromvalue_28:variant, optionalfromvalue_29:variant, optionalfromvalue_30:variant, optionalfromvalue_31:variant, optionalfromvalue_32:variant, optionalfromvalue_33:variant, optionalfromvalue_34:variant, optionalfromvalue_35:variant, optionalfromvalue_36:variant, optionalfromvalue_37:variant, optionalfromvalue_38:variant, optionalfromvalue_39:variant, optionalfromvalue_40:variant, optionalfromvalue_41:variant, optionalfromvalue_42:variant, optionalfromvalue_43:variant, optionalfromvalue_44:variant, optionalfromvalue_45:variant, optionalfromvalue_46:variant, optionalfromvalue_47:variant, optionalfromvalue_48:variant, optionalfromvalue_49:variant, optionalfromvalue_50:variant, optionalfromvalue_51:variant, optionalfromvalue_52:variant, optionalfromvalue_53:variant, optionalfromvalue_54:variant, optionalfromvalue_55:variant, optionalfromvalue_56:variant, optionalfromvalue_57:variant, optionalfromvalue_58:variant, optionalfromvalue_59:variant, optionalfromvalue_60:variant, optionalfromvalue_61:variant, optionalfromvalue_62:variant, optionalfromvalue_63:variant, optionalfromvalue_64:variant, optionalfromvalue_65:variant; is_stateful=true>\n\t [[node test_model3_2/bidirectional_15/backward_lstm_15/StatefulPartitionedCall (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\n\t [[metrics/precision/assert_greater_equal/Assert/AssertGuard/pivot_f/_26/_83]]\n  (1) Invalid argument:  7 inputs specified of 11 inputs in Op while building NodeDef 'cond_65/then/_0' using Op<name=cond_true_179844_rewritten; signature=transpose_inputs:float, expanddims_init_h:float, expanddims_1_init_c:float, split_readvariableop_resource:resource, split_1_readvariableop_resource:resource, zeros_like_readvariableop_resource:resource, cast_stopgradient:bool, optionalfromvalue_60_deleteiterator_input_iterator:resource, optionalfromvalue_60_deleteiterator_input_iterator_1:variant, optionalfromvalue_60_deleteiterator_2_input_iterator:resource, optionalfromvalue_60_deleteiterator_2_input_iterator_1:variant -> identity:float, identity_1:float, identity_2:float, identity_3:float, identity_4:float, optionalfromvalue:variant, optionalfromvalue_1:variant, optionalfromvalue_2:variant, optionalfromvalue_3:variant, optionalfromvalue_4:variant, optionalfromvalue_5:variant, optionalfromvalue_6:variant, optionalfromvalue_7:variant, optionalfromvalue_8:variant, optionalfromvalue_9:variant, optionalfromvalue_10:variant, optionalfromvalue_11:variant, optionalfromvalue_12:variant, optionalfromvalue_13:variant, optionalfromvalue_14:variant, optionalfromvalue_15:variant, optionalfromvalue_16:variant, optionalfromvalue_17:variant, optionalfromvalue_18:variant, optionalfromvalue_19:variant, optionalfromvalue_20:variant, optionalfromvalue_21:variant, optionalfromvalue_22:variant, optionalfromvalue_23:variant, optionalfromvalue_24:variant, optionalfromvalue_25:variant, optionalfromvalue_26:variant, optionalfromvalue_27:variant, optionalfromvalue_28:variant, optionalfromvalue_29:variant, optionalfromvalue_30:variant, optionalfromvalue_31:variant, optionalfromvalue_32:variant, optionalfromvalue_33:variant, optionalfromvalue_34:variant, optionalfromvalue_35:variant, optionalfromvalue_36:variant, optionalfromvalue_37:variant, optionalfromvalue_38:variant, optionalfromvalue_39:variant, optionalfromvalue_40:variant, optionalfromvalue_41:variant, optionalfromvalue_42:variant, optionalfromvalue_43:variant, optionalfromvalue_44:variant, optionalfromvalue_45:variant, optionalfromvalue_46:variant, optionalfromvalue_47:variant, optionalfromvalue_48:variant, optionalfromvalue_49:variant, optionalfromvalue_50:variant, optionalfromvalue_51:variant, optionalfromvalue_52:variant, optionalfromvalue_53:variant, optionalfromvalue_54:variant, optionalfromvalue_55:variant, optionalfromvalue_56:variant, optionalfromvalue_57:variant, optionalfromvalue_58:variant, optionalfromvalue_59:variant, optionalfromvalue_60:variant, optionalfromvalue_61:variant, optionalfromvalue_62:variant, optionalfromvalue_63:variant, optionalfromvalue_64:variant, optionalfromvalue_65:variant; is_stateful=true>\n\t [[node test_model3_2/bidirectional_15/backward_lstm_15/StatefulPartitionedCall (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_184030]\n\nFunction call stack:\ndistributed_function -> distributed_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-39095b12f153>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_char_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_char_valid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  7 inputs specified of 11 inputs in Op while building NodeDef 'cond_65/then/_0' using Op<name=cond_true_179844_rewritten; signature=transpose_inputs:float, expanddims_init_h:float, expanddims_1_init_c:float, split_readvariableop_resource:resource, split_1_readvariableop_resource:resource, zeros_like_readvariableop_resource:resource, cast_stopgradient:bool, optionalfromvalue_60_deleteiterator_input_iterator:resource, optionalfromvalue_60_deleteiterator_input_iterator_1:variant, optionalfromvalue_60_deleteiterator_2_input_iterator:resource, optionalfromvalue_60_deleteiterator_2_input_iterator_1:variant -> identity:float, identity_1:float, identity_2:float, identity_3:float, identity_4:float, optionalfromvalue:variant, optionalfromvalue_1:variant, optionalfromvalue_2:variant, optionalfromvalue_3:variant, optionalfromvalue_4:variant, optionalfromvalue_5:variant, optionalfromvalue_6:variant, optionalfromvalue_7:variant, optionalfromvalue_8:variant, optionalfromvalue_9:variant, optionalfromvalue_10:variant, optionalfromvalue_11:variant, optionalfromvalue_12:variant, optionalfromvalue_13:variant, optionalfromvalue_14:variant, optionalfromvalue_15:variant, optionalfromvalue_16:variant, optionalfromvalue_17:variant, optionalfromvalue_18:variant, optionalfromvalue_19:variant, optionalfromvalue_20:variant, optionalfromvalue_21:variant, optionalfromvalue_22:variant, optionalfromvalue_23:variant, optionalfromvalue_24:variant, optionalfromvalue_25:variant, optionalfromvalue_26:variant, optionalfromvalue_27:variant, optionalfromvalue_28:variant, optionalfromvalue_29:variant, optionalfromvalue_30:variant, optionalfromvalue_31:variant, optionalfromvalue_32:variant, optionalfromvalue_33:variant, optionalfromvalue_34:variant, optionalfromvalue_35:variant, optionalfromvalue_36:variant, optionalfromvalue_37:variant, optionalfromvalue_38:variant, optionalfromvalue_39:variant, optionalfromvalue_40:variant, optionalfromvalue_41:variant, optionalfromvalue_42:variant, optionalfromvalue_43:variant, optionalfromvalue_44:variant, optionalfromvalue_45:variant, optionalfromvalue_46:variant, optionalfromvalue_47:variant, optionalfromvalue_48:variant, optionalfromvalue_49:variant, optionalfromvalue_50:variant, optionalfromvalue_51:variant, optionalfromvalue_52:variant, optionalfromvalue_53:variant, optionalfromvalue_54:variant, optionalfromvalue_55:variant, optionalfromvalue_56:variant, optionalfromvalue_57:variant, optionalfromvalue_58:variant, optionalfromvalue_59:variant, optionalfromvalue_60:variant, optionalfromvalue_61:variant, optionalfromvalue_62:variant, optionalfromvalue_63:variant, optionalfromvalue_64:variant, optionalfromvalue_65:variant; is_stateful=true>\n\t [[node test_model3_2/bidirectional_15/backward_lstm_15/StatefulPartitionedCall (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\n\t [[metrics/precision/assert_greater_equal/Assert/AssertGuard/pivot_f/_26/_83]]\n  (1) Invalid argument:  7 inputs specified of 11 inputs in Op while building NodeDef 'cond_65/then/_0' using Op<name=cond_true_179844_rewritten; signature=transpose_inputs:float, expanddims_init_h:float, expanddims_1_init_c:float, split_readvariableop_resource:resource, split_1_readvariableop_resource:resource, zeros_like_readvariableop_resource:resource, cast_stopgradient:bool, optionalfromvalue_60_deleteiterator_input_iterator:resource, optionalfromvalue_60_deleteiterator_input_iterator_1:variant, optionalfromvalue_60_deleteiterator_2_input_iterator:resource, optionalfromvalue_60_deleteiterator_2_input_iterator_1:variant -> identity:float, identity_1:float, identity_2:float, identity_3:float, identity_4:float, optionalfromvalue:variant, optionalfromvalue_1:variant, optionalfromvalue_2:variant, optionalfromvalue_3:variant, optionalfromvalue_4:variant, optionalfromvalue_5:variant, optionalfromvalue_6:variant, optionalfromvalue_7:variant, optionalfromvalue_8:variant, optionalfromvalue_9:variant, optionalfromvalue_10:variant, optionalfromvalue_11:variant, optionalfromvalue_12:variant, optionalfromvalue_13:variant, optionalfromvalue_14:variant, optionalfromvalue_15:variant, optionalfromvalue_16:variant, optionalfromvalue_17:variant, optionalfromvalue_18:variant, optionalfromvalue_19:variant, optionalfromvalue_20:variant, optionalfromvalue_21:variant, optionalfromvalue_22:variant, optionalfromvalue_23:variant, optionalfromvalue_24:variant, optionalfromvalue_25:variant, optionalfromvalue_26:variant, optionalfromvalue_27:variant, optionalfromvalue_28:variant, optionalfromvalue_29:variant, optionalfromvalue_30:variant, optionalfromvalue_31:variant, optionalfromvalue_32:variant, optionalfromvalue_33:variant, optionalfromvalue_34:variant, optionalfromvalue_35:variant, optionalfromvalue_36:variant, optionalfromvalue_37:variant, optionalfromvalue_38:variant, optionalfromvalue_39:variant, optionalfromvalue_40:variant, optionalfromvalue_41:variant, optionalfromvalue_42:variant, optionalfromvalue_43:variant, optionalfromvalue_44:variant, optionalfromvalue_45:variant, optionalfromvalue_46:variant, optionalfromvalue_47:variant, optionalfromvalue_48:variant, optionalfromvalue_49:variant, optionalfromvalue_50:variant, optionalfromvalue_51:variant, optionalfromvalue_52:variant, optionalfromvalue_53:variant, optionalfromvalue_54:variant, optionalfromvalue_55:variant, optionalfromvalue_56:variant, optionalfromvalue_57:variant, optionalfromvalue_58:variant, optionalfromvalue_59:variant, optionalfromvalue_60:variant, optionalfromvalue_61:variant, optionalfromvalue_62:variant, optionalfromvalue_63:variant, optionalfromvalue_64:variant, optionalfromvalue_65:variant; is_stateful=true>\n\t [[node test_model3_2/bidirectional_15/backward_lstm_15/StatefulPartitionedCall (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_184030]\n\nFunction call stack:\ndistributed_function -> distributed_function\n"
     ]
    }
   ],
   "source": [
    "model3.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss,pre,recall = model3.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n",
    "\n",
    "# print(model3.evaluate([X_test,X_char_test], y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 5. Conv2D\n",
    "### Embedding 사용하여 차원 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel5 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=sentence_len, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5), mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv2D =tf.keras.layers.Conv2D(kernel_size=(3,3), filters=30, padding='same',activation='tanh', strides=1, input_shape=(70,52,30))\n",
    "        self.CharMaxpooling =  tf.keras.layers.MaxPooling2D(pool_size=(1,52))\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        chrc = self.CharMaxpooling(chrc)\n",
    "        if wrd.shape[1]!=None:\n",
    "            reshaper = tf.keras.layers.Reshape((wrd.shape[1], 30))\n",
    "        else:\n",
    "            reshaper = tf.keras.layers.Reshape((-1, 30))\n",
    "        chrc = reshaper(chrc)\n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5 = TestModel5(vocab_size,tag_size)\n",
    "aaa =model5([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model5_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_46 (Embedding)     (None, None, 128)         512000    \n_________________________________________________________________\nembedding_47 (Embedding)     (None, None, 52, 30)      2790      \n_________________________________________________________________\ndropout_26 (Dropout)         (None, None, 52, 30)      0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, None, 52, 30)      8130      \n_________________________________________________________________\nmax_pooling2d_14 (MaxPooling (None, None, 1, 30)       0         \n_________________________________________________________________\nbidirectional_24 (Bidirectio (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_43 (TimeDis (None, None, 10)          5130      \n=================================================================\nTotal params: 1,377,970\nTrainable params: 1,377,970\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model5.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 16s 1ms/sample - loss: 0.1245 - precision: 0.8968 - recall: 0.7969 - val_loss: 0.0840 - val_precision: 0.9612 - val_recall: 0.8287\nEpoch 2/8\n14041/14041 [==============================] - 8s 583us/sample - loss: 0.0525 - precision: 0.9600 - recall: 0.8919 - val_loss: 0.0469 - val_precision: 0.9686 - val_recall: 0.9148\nEpoch 3/8\n14041/14041 [==============================] - 8s 581us/sample - loss: 0.0271 - precision: 0.9768 - recall: 0.9494 - val_loss: 0.0284 - val_precision: 0.9812 - val_recall: 0.9521\nEpoch 4/8\n14041/14041 [==============================] - 8s 583us/sample - loss: 0.0176 - precision: 0.9838 - recall: 0.9671 - val_loss: 0.0245 - val_precision: 0.9790 - val_recall: 0.9607\nEpoch 5/8\n14041/14041 [==============================] - 8s 578us/sample - loss: 0.0139 - precision: 0.9868 - recall: 0.9743 - val_loss: 0.0232 - val_precision: 0.9798 - val_recall: 0.9627\nEpoch 6/8\n14041/14041 [==============================] - 8s 580us/sample - loss: 0.0118 - precision: 0.9884 - recall: 0.9779 - val_loss: 0.0235 - val_precision: 0.9780 - val_recall: 0.9632\nEpoch 7/8\n14041/14041 [==============================] - 8s 577us/sample - loss: 0.0101 - precision: 0.9898 - recall: 0.9816 - val_loss: 0.0219 - val_precision: 0.9787 - val_recall: 0.9654\nEpoch 8/8\n14041/14041 [==============================] - 8s 581us/sample - loss: 0.0087 - precision: 0.9910 - recall: 0.9844 - val_loss: 0.0232 - val_precision: 0.9773 - val_recall: 0.9663\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21efb8a9808>"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "model5.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9572\n\n 테스트 정확도 : 0.9708\n"
    }
   ],
   "source": [
    "loss,pre,recall = model5.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================================================\n",
    "# 모델 6. SeparableConv1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel6 (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.WordEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=sentence_len, output_dim=128, mask_zero=True)\n",
    "        self.CharEmbedding = tf.keras.layers.Embedding(len(char_to_index),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5), mask_zero=True)\n",
    "        self.CharDropout1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.CharConv2D =tf.keras.layers.SeparableConv2D(kernel_size=(1,52), filters=30,activation='tanh', strides=1, input_shape=(70,52,30))\n",
    "        # self.CharMaxpooling =  tf.keras.layers.MaxPooling2D(pool_size=(1, 52))\n",
    "        self.CharFlatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp):\n",
    "        wrd = self.WordEmbedding(inp[0])\n",
    "        chrc = self.CharEmbedding(inp[1])\n",
    "        chrc = self.CharDropout1(chrc)\n",
    "        chrc = self.CharConv2D(chrc)\n",
    "        # chrc = self.CharMaxpooling(chrc)\n",
    "        chrc = self.CharFlatten(chrc)\n",
    "  \n",
    "        x = tf.keras.layers.concatenate([wrd,chrc])\n",
    "        x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([None, None, 52])"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "character_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'test_model6_9/Identity:0' shape=(None, None, 10) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "model6 = TestModel6(vocab_size,tag_size)\n",
    "model6([words_input,character_input])\n",
    "# modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test_model6_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_32 (Embedding)     (None, None, 128)         512000    \n_________________________________________________________________\nembedding_33 (Embedding)     (None, None, 52, 30)      2790      \n_________________________________________________________________\ndropout_19 (Dropout)         (None, None, 52, 30)      0         \n_________________________________________________________________\nseparable_conv2d_9 (Separabl (None, None, 1, 30)       2490      \n_________________________________________________________________\ntime_distributed_35 (TimeDis (None, None, 30)          0         \n_________________________________________________________________\nbidirectional_17 (Bidirectio (None, None, 512)         849920    \n_________________________________________________________________\ntime_distributed_36 (TimeDis (None, None, 10)          5130      \n=================================================================\nTotal params: 1,372,330\nTrainable params: 1,372,330\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model6.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 19s 1ms/sample - loss: 0.1363 - precision: 0.8944 - recall: 0.7735 - val_loss: 0.0876 - val_precision: 0.9704 - val_recall: 0.8203\nEpoch 2/8\n14041/14041 [==============================] - 8s 552us/sample - loss: 0.0529 - precision: 0.9655 - recall: 0.8882 - val_loss: 0.0403 - val_precision: 0.9760 - val_recall: 0.9265\nEpoch 3/8\n14041/14041 [==============================] - 8s 556us/sample - loss: 0.0280 - precision: 0.9780 - recall: 0.9467 - val_loss: 0.0282 - val_precision: 0.9784 - val_recall: 0.9542\nEpoch 4/8\n14041/14041 [==============================] - 8s 551us/sample - loss: 0.0190 - precision: 0.9825 - recall: 0.9654 - val_loss: 0.0241 - val_precision: 0.9809 - val_recall: 0.9602\nEpoch 5/8\n14041/14041 [==============================] - 8s 548us/sample - loss: 0.0156 - precision: 0.9849 - recall: 0.9718 - val_loss: 0.0241 - val_precision: 0.9799 - val_recall: 0.9636\nEpoch 6/8\n14041/14041 [==============================] - 8s 552us/sample - loss: 0.0132 - precision: 0.9869 - recall: 0.9765 - val_loss: 0.0265 - val_precision: 0.9745 - val_recall: 0.9577\nEpoch 7/8\n14041/14041 [==============================] - 8s 550us/sample - loss: 0.0111 - precision: 0.9885 - recall: 0.9800 - val_loss: 0.0222 - val_precision: 0.9785 - val_recall: 0.9667\nEpoch 8/8\n14041/14041 [==============================] - 8s 550us/sample - loss: 0.0098 - precision: 0.9897 - recall: 0.9827 - val_loss: 0.0215 - val_precision: 0.9798 - val_recall: 0.9683\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21ed5c6abc8>"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "model6.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 f-1 score: 0.9601\n\n 테스트 정확도 : 0.9723\n"
    }
   ],
   "source": [
    "loss,pre,recall = model6.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "f1_val = 2*(pre*recall)/(pre+recall)\n",
    "print(\"\\n 테스트 f-1 score: %.4f\" % f1_val)\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (1-loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595981209681",
   "display_name": "Python 3.7.7 64-bit ('shyun': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}