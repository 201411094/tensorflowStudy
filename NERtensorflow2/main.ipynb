{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input,Embedding,TimeDistributed,Dropout,Conv1D,Dense,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate,GRU\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f=open(filename)\n",
    "    sentences=[]\n",
    "    sentence=[]\n",
    "    \n",
    "    for line in f:\n",
    "        if(len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\"):\n",
    "            if(len(sentence)>0):\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "                #print('1', sentence)\n",
    "            continue\n",
    "        splits=line.split(' ')\n",
    "        splits[-1] = re.sub(r'\\n','',splits[-1])  ## 191106 추가\n",
    "        sentence.append([splits[0],splits[-1]])\n",
    "        #print('2', sentence)\n",
    "    if(len(sentence)>0):\n",
    "        sentences.append(sentence)\n",
    "        #print('3', sentence)\n",
    "        sentence=[]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 단어와 개체명만 뽑아 문장으로 만들어 append 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainSentences=readfile(\"train.txt\")\n",
    "validSentences=readfile(\"valid.txt\")\n",
    "testSentences=readfile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['EU', 'B-ORG'],\n ['rejects', 'O'],\n ['German', 'B-MISC'],\n ['call', 'O'],\n ['to', 'O'],\n ['boycott', 'O'],\n ['British', 'B-MISC'],\n ['lamb', 'O'],\n ['.', 'O']]"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">trainSentences(14041,), devSentences(3250,), testSentences(3453,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Character 추출 및 단어, Character, label을 리스트로 구성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars=[c for c in data[0]]            # Character 분리\n",
    "            Sentences[i][j]=[data[0],chars,data[-1]] # 단어, Chracter, NER을 리스트로\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences=addCharInformation(trainSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[['EU', ['E', 'U'], 'B-ORG'],\n  ['rejects', ['r', 'e', 'j', 'e', 'c', 't', 's'], 'O'],\n  ['German', ['G', 'e', 'r', 'm', 'a', 'n'], 'B-MISC'],\n  ['call', ['c', 'a', 'l', 'l'], 'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['boycott', ['b', 'o', 'y', 'c', 'o', 't', 't'], 'O'],\n  ['British', ['B', 'r', 'i', 't', 'i', 's', 'h'], 'B-MISC'],\n  ['lamb', ['l', 'a', 'm', 'b'], 'O'],\n  ['.', ['.'], 'O']],\n [['Peter', ['P', 'e', 't', 'e', 'r'], 'B-PER'],\n  ['Blackburn', ['B', 'l', 'a', 'c', 'k', 'b', 'u', 'r', 'n'], 'I-PER']],\n [['BRUSSELS', ['B', 'R', 'U', 'S', 'S', 'E', 'L', 'S'], 'B-LOC'],\n  ['1996-08-22', ['1', '9', '9', '6', '-', '0', '8', '-', '2', '2'], 'O']],\n [['The', ['T', 'h', 'e'], 'O'],\n  ['European', ['E', 'u', 'r', 'o', 'p', 'e', 'a', 'n'], 'B-ORG'],\n  ['Commission', ['C', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n'], 'I-ORG'],\n  ['said', ['s', 'a', 'i', 'd'], 'O'],\n  ['on', ['o', 'n'], 'O'],\n  ['Thursday', ['T', 'h', 'u', 'r', 's', 'd', 'a', 'y'], 'O'],\n  ['it', ['i', 't'], 'O'],\n  ['disagreed', ['d', 'i', 's', 'a', 'g', 'r', 'e', 'e', 'd'], 'O'],\n  ['with', ['w', 'i', 't', 'h'], 'O'],\n  ['German', ['G', 'e', 'r', 'm', 'a', 'n'], 'B-MISC'],\n  ['advice', ['a', 'd', 'v', 'i', 'c', 'e'], 'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['consumers', ['c', 'o', 'n', 's', 'u', 'm', 'e', 'r', 's'], 'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['shun', ['s', 'h', 'u', 'n'], 'O'],\n  ['British', ['B', 'r', 'i', 't', 'i', 's', 'h'], 'B-MISC'],\n  ['lamb', ['l', 'a', 'm', 'b'], 'O'],\n  ['until', ['u', 'n', 't', 'i', 'l'], 'O'],\n  ['scientists', ['s', 'c', 'i', 'e', 'n', 't', 'i', 's', 't', 's'], 'O'],\n  ['determine', ['d', 'e', 't', 'e', 'r', 'm', 'i', 'n', 'e'], 'O'],\n  ['whether', ['w', 'h', 'e', 't', 'h', 'e', 'r'], 'O'],\n  ['mad', ['m', 'a', 'd'], 'O'],\n  ['cow', ['c', 'o', 'w'], 'O'],\n  ['disease', ['d', 'i', 's', 'e', 'a', 's', 'e'], 'O'],\n  ['can', ['c', 'a', 'n'], 'O'],\n  ['be', ['b', 'e'], 'O'],\n  ['transmitted',\n   ['t', 'r', 'a', 'n', 's', 'm', 'i', 't', 't', 'e', 'd'],\n   'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['sheep', ['s', 'h', 'e', 'e', 'p'], 'O'],\n  ['.', ['.'], 'O']],\n [['Germany', ['G', 'e', 'r', 'm', 'a', 'n', 'y'], 'B-LOC'],\n  [\"'s\", [\"'\", 's'], 'O'],\n  ['representative',\n   ['r', 'e', 'p', 'r', 'e', 's', 'e', 'n', 't', 'a', 't', 'i', 'v', 'e'],\n   'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['the', ['t', 'h', 'e'], 'O'],\n  ['European', ['E', 'u', 'r', 'o', 'p', 'e', 'a', 'n'], 'B-ORG'],\n  ['Union', ['U', 'n', 'i', 'o', 'n'], 'I-ORG'],\n  [\"'s\", [\"'\", 's'], 'O'],\n  ['veterinary', ['v', 'e', 't', 'e', 'r', 'i', 'n', 'a', 'r', 'y'], 'O'],\n  ['committee', ['c', 'o', 'm', 'm', 'i', 't', 't', 'e', 'e'], 'O'],\n  ['Werner', ['W', 'e', 'r', 'n', 'e', 'r'], 'B-PER'],\n  ['Zwingmann', ['Z', 'w', 'i', 'n', 'g', 'm', 'a', 'n', 'n'], 'I-PER'],\n  ['said', ['s', 'a', 'i', 'd'], 'O'],\n  ['on', ['o', 'n'], 'O'],\n  ['Wednesday', ['W', 'e', 'd', 'n', 'e', 's', 'd', 'a', 'y'], 'O'],\n  ['consumers', ['c', 'o', 'n', 's', 'u', 'm', 'e', 'r', 's'], 'O'],\n  ['should', ['s', 'h', 'o', 'u', 'l', 'd'], 'O'],\n  ['buy', ['b', 'u', 'y'], 'O'],\n  ['sheepmeat', ['s', 'h', 'e', 'e', 'p', 'm', 'e', 'a', 't'], 'O'],\n  ['from', ['f', 'r', 'o', 'm'], 'O'],\n  ['countries', ['c', 'o', 'u', 'n', 't', 'r', 'i', 'e', 's'], 'O'],\n  ['other', ['o', 't', 'h', 'e', 'r'], 'O'],\n  ['than', ['t', 'h', 'a', 'n'], 'O'],\n  ['Britain', ['B', 'r', 'i', 't', 'a', 'i', 'n'], 'B-LOC'],\n  ['until', ['u', 'n', 't', 'i', 'l'], 'O'],\n  ['the', ['t', 'h', 'e'], 'O'],\n  ['scientific', ['s', 'c', 'i', 'e', 'n', 't', 'i', 'f', 'i', 'c'], 'O'],\n  ['advice', ['a', 'd', 'v', 'i', 'c', 'e'], 'O'],\n  ['was', ['w', 'a', 's'], 'O'],\n  ['clearer', ['c', 'l', 'e', 'a', 'r', 'e', 'r'], 'O'],\n  ['.', ['.'], 'O']],\n [['\"', ['\"'], 'O'],\n  ['We', ['W', 'e'], 'O'],\n  ['do', ['d', 'o'], 'O'],\n  [\"n't\", ['n', \"'\", 't'], 'O'],\n  ['support', ['s', 'u', 'p', 'p', 'o', 'r', 't'], 'O'],\n  ['any', ['a', 'n', 'y'], 'O'],\n  ['such', ['s', 'u', 'c', 'h'], 'O'],\n  ['recommendation',\n   ['r', 'e', 'c', 'o', 'm', 'm', 'e', 'n', 'd', 'a', 't', 'i', 'o', 'n'],\n   'O'],\n  ['because', ['b', 'e', 'c', 'a', 'u', 's', 'e'], 'O'],\n  ['we', ['w', 'e'], 'O'],\n  ['do', ['d', 'o'], 'O'],\n  [\"n't\", ['n', \"'\", 't'], 'O'],\n  ['see', ['s', 'e', 'e'], 'O'],\n  ['any', ['a', 'n', 'y'], 'O'],\n  ['grounds', ['g', 'r', 'o', 'u', 'n', 'd', 's'], 'O'],\n  ['for', ['f', 'o', 'r'], 'O'],\n  ['it', ['i', 't'], 'O'],\n  [',', [','], 'O'],\n  ['\"', ['\"'], 'O'],\n  ['the', ['t', 'h', 'e'], 'O'],\n  ['Commission', ['C', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n'], 'B-ORG'],\n  [\"'s\", [\"'\", 's'], 'O'],\n  ['chief', ['c', 'h', 'i', 'e', 'f'], 'O'],\n  ['spokesman', ['s', 'p', 'o', 'k', 'e', 's', 'm', 'a', 'n'], 'O'],\n  ['Nikolaus', ['N', 'i', 'k', 'o', 'l', 'a', 'u', 's'], 'B-PER'],\n  ['van', ['v', 'a', 'n'], 'I-PER'],\n  ['der', ['d', 'e', 'r'], 'I-PER'],\n  ['Pas', ['P', 'a', 's'], 'I-PER'],\n  ['told', ['t', 'o', 'l', 'd'], 'O'],\n  ['a', ['a'], 'O'],\n  ['news', ['n', 'e', 'w', 's'], 'O'],\n  ['briefing', ['b', 'r', 'i', 'e', 'f', 'i', 'n', 'g'], 'O'],\n  ['.', ['.'], 'O']],\n [['He', ['H', 'e'], 'O'],\n  ['said', ['s', 'a', 'i', 'd'], 'O'],\n  ['further', ['f', 'u', 'r', 't', 'h', 'e', 'r'], 'O'],\n  ['scientific', ['s', 'c', 'i', 'e', 'n', 't', 'i', 'f', 'i', 'c'], 'O'],\n  ['study', ['s', 't', 'u', 'd', 'y'], 'O'],\n  ['was', ['w', 'a', 's'], 'O'],\n  ['required', ['r', 'e', 'q', 'u', 'i', 'r', 'e', 'd'], 'O'],\n  ['and', ['a', 'n', 'd'], 'O'],\n  ['if', ['i', 'f'], 'O'],\n  ['it', ['i', 't'], 'O'],\n  ['was', ['w', 'a', 's'], 'O'],\n  ['found', ['f', 'o', 'u', 'n', 'd'], 'O'],\n  ['that', ['t', 'h', 'a', 't'], 'O'],\n  ['action', ['a', 'c', 't', 'i', 'o', 'n'], 'O'],\n  ['was', ['w', 'a', 's'], 'O'],\n  ['needed', ['n', 'e', 'e', 'd', 'e', 'd'], 'O'],\n  ['it', ['i', 't'], 'O'],\n  ['should', ['s', 'h', 'o', 'u', 'l', 'd'], 'O'],\n  ['be', ['b', 'e'], 'O'],\n  ['taken', ['t', 'a', 'k', 'e', 'n'], 'O'],\n  ['by', ['b', 'y'], 'O'],\n  ['the', ['t', 'h', 'e'], 'O'],\n  ['European', ['E', 'u', 'r', 'o', 'p', 'e', 'a', 'n'], 'B-ORG'],\n  ['Union', ['U', 'n', 'i', 'o', 'n'], 'I-ORG'],\n  ['.', ['.'], 'O']],\n [['He', ['H', 'e'], 'O'],\n  ['said', ['s', 'a', 'i', 'd'], 'O'],\n  ['a', ['a'], 'O'],\n  ['proposal', ['p', 'r', 'o', 'p', 'o', 's', 'a', 'l'], 'O'],\n  ['last', ['l', 'a', 's', 't'], 'O'],\n  ['month', ['m', 'o', 'n', 't', 'h'], 'O'],\n  ['by', ['b', 'y'], 'O'],\n  ['EU', ['E', 'U'], 'B-ORG'],\n  ['Farm', ['F', 'a', 'r', 'm'], 'O'],\n  ['Commissioner',\n   ['C', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n', 'e', 'r'],\n   'O'],\n  ['Franz', ['F', 'r', 'a', 'n', 'z'], 'B-PER'],\n  ['Fischler', ['F', 'i', 's', 'c', 'h', 'l', 'e', 'r'], 'I-PER'],\n  ['to', ['t', 'o'], 'O'],\n  ['ban', ['b', 'a', 'n'], 'O'],\n  ['sheep', ['s', 'h', 'e', 'e', 'p'], 'O'],\n  ['brains', ['b', 'r', 'a', 'i', 'n', 's'], 'O'],\n  [',', [','], 'O'],\n  ['spleens', ['s', 'p', 'l', 'e', 'e', 'n', 's'], 'O'],\n  ['and', ['a', 'n', 'd'], 'O'],\n  ['spinal', ['s', 'p', 'i', 'n', 'a', 'l'], 'O'],\n  ['cords', ['c', 'o', 'r', 'd', 's'], 'O'],\n  ['from', ['f', 'r', 'o', 'm'], 'O'],\n  ['the', ['t', 'h', 'e'], 'O'],\n  ['human', ['h', 'u', 'm', 'a', 'n'], 'O'],\n  ['and', ['a', 'n', 'd'], 'O'],\n  ['animal', ['a', 'n', 'i', 'm', 'a', 'l'], 'O'],\n  ['food', ['f', 'o', 'o', 'd'], 'O'],\n  ['chains', ['c', 'h', 'a', 'i', 'n', 's'], 'O'],\n  ['was', ['w', 'a', 's'], 'O'],\n  ['a', ['a'], 'O'],\n  ['highly', ['h', 'i', 'g', 'h', 'l', 'y'], 'O'],\n  ['specific', ['s', 'p', 'e', 'c', 'i', 'f', 'i', 'c'], 'O'],\n  ['and', ['a', 'n', 'd'], 'O'],\n  ['precautionary',\n   ['p', 'r', 'e', 'c', 'a', 'u', 't', 'i', 'o', 'n', 'a', 'r', 'y'],\n   'O'],\n  ['move', ['m', 'o', 'v', 'e'], 'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['protect', ['p', 'r', 'o', 't', 'e', 'c', 't'], 'O'],\n  ['human', ['h', 'u', 'm', 'a', 'n'], 'O'],\n  ['health', ['h', 'e', 'a', 'l', 't', 'h'], 'O'],\n  ['.', ['.'], 'O']],\n [['Fischler', ['F', 'i', 's', 'c', 'h', 'l', 'e', 'r'], 'B-PER'],\n  ['proposed', ['p', 'r', 'o', 'p', 'o', 's', 'e', 'd'], 'O'],\n  ['EU-wide', ['E', 'U', '-', 'w', 'i', 'd', 'e'], 'B-MISC'],\n  ['measures', ['m', 'e', 'a', 's', 'u', 'r', 'e', 's'], 'O'],\n  ['after', ['a', 'f', 't', 'e', 'r'], 'O'],\n  ['reports', ['r', 'e', 'p', 'o', 'r', 't', 's'], 'O'],\n  ['from', ['f', 'r', 'o', 'm'], 'O'],\n  ['Britain', ['B', 'r', 'i', 't', 'a', 'i', 'n'], 'B-LOC'],\n  ['and', ['a', 'n', 'd'], 'O'],\n  ['France', ['F', 'r', 'a', 'n', 'c', 'e'], 'B-LOC'],\n  ['that', ['t', 'h', 'a', 't'], 'O'],\n  ['under', ['u', 'n', 'd', 'e', 'r'], 'O'],\n  ['laboratory', ['l', 'a', 'b', 'o', 'r', 'a', 't', 'o', 'r', 'y'], 'O'],\n  ['conditions', ['c', 'o', 'n', 'd', 'i', 't', 'i', 'o', 'n', 's'], 'O'],\n  ['sheep', ['s', 'h', 'e', 'e', 'p'], 'O'],\n  ['could', ['c', 'o', 'u', 'l', 'd'], 'O'],\n  ['contract', ['c', 'o', 'n', 't', 'r', 'a', 'c', 't'], 'O'],\n  ['Bovine', ['B', 'o', 'v', 'i', 'n', 'e'], 'B-MISC'],\n  ['Spongiform', ['S', 'p', 'o', 'n', 'g', 'i', 'f', 'o', 'r', 'm'], 'I-MISC'],\n  ['Encephalopathy',\n   ['E', 'n', 'c', 'e', 'p', 'h', 'a', 'l', 'o', 'p', 'a', 't', 'h', 'y'],\n   'I-MISC'],\n  ['(', ['('], 'O'],\n  ['BSE', ['B', 'S', 'E'], 'B-MISC'],\n  [')', [')'], 'O'],\n  ['--', ['-', '-'], 'O'],\n  ['mad', ['m', 'a', 'd'], 'O'],\n  ['cow', ['c', 'o', 'w'], 'O'],\n  ['disease', ['d', 'i', 's', 'e', 'a', 's', 'e'], 'O'],\n  ['.', ['.'], 'O']],\n [['But', ['B', 'u', 't'], 'O'],\n  ['Fischler', ['F', 'i', 's', 'c', 'h', 'l', 'e', 'r'], 'B-PER'],\n  ['agreed', ['a', 'g', 'r', 'e', 'e', 'd'], 'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['review', ['r', 'e', 'v', 'i', 'e', 'w'], 'O'],\n  ['his', ['h', 'i', 's'], 'O'],\n  ['proposal', ['p', 'r', 'o', 'p', 'o', 's', 'a', 'l'], 'O'],\n  ['after', ['a', 'f', 't', 'e', 'r'], 'O'],\n  ['the', ['t', 'h', 'e'], 'O'],\n  ['EU', ['E', 'U'], 'B-ORG'],\n  [\"'s\", [\"'\", 's'], 'O'],\n  ['standing', ['s', 't', 'a', 'n', 'd', 'i', 'n', 'g'], 'O'],\n  ['veterinary', ['v', 'e', 't', 'e', 'r', 'i', 'n', 'a', 'r', 'y'], 'O'],\n  ['committee', ['c', 'o', 'm', 'm', 'i', 't', 't', 'e', 'e'], 'O'],\n  [',', [','], 'O'],\n  ['mational', ['m', 'a', 't', 'i', 'o', 'n', 'a', 'l'], 'O'],\n  ['animal', ['a', 'n', 'i', 'm', 'a', 'l'], 'O'],\n  ['health', ['h', 'e', 'a', 'l', 't', 'h'], 'O'],\n  ['officials', ['o', 'f', 'f', 'i', 'c', 'i', 'a', 'l', 's'], 'O'],\n  [',', [','], 'O'],\n  ['questioned', ['q', 'u', 'e', 's', 't', 'i', 'o', 'n', 'e', 'd'], 'O'],\n  ['if', ['i', 'f'], 'O'],\n  ['such', ['s', 'u', 'c', 'h'], 'O'],\n  ['action', ['a', 'c', 't', 'i', 'o', 'n'], 'O'],\n  ['was', ['w', 'a', 's'], 'O'],\n  ['justified', ['j', 'u', 's', 't', 'i', 'f', 'i', 'e', 'd'], 'O'],\n  ['as', ['a', 's'], 'O'],\n  ['there', ['t', 'h', 'e', 'r', 'e'], 'O'],\n  ['was', ['w', 'a', 's'], 'O'],\n  ['only', ['o', 'n', 'l', 'y'], 'O'],\n  ['a', ['a'], 'O'],\n  ['slight', ['s', 'l', 'i', 'g', 'h', 't'], 'O'],\n  ['risk', ['r', 'i', 's', 'k'], 'O'],\n  ['to', ['t', 'o'], 'O'],\n  ['human', ['h', 'u', 'm', 'a', 'n'], 'O'],\n  ['health', ['h', 'e', 'a', 'l', 't', 'h'], 'O'],\n  ['.', ['.'], 'O']]]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "trainSentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSentences=addCharInformation(validSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentences=addCharInformation(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 단어와 label의 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet=set()\n",
    "words={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset in [trainSentences,validSentences,testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token,char,label in sentence:\n",
    "            #print(token,char,label)\n",
    "            labelSet.add(label)       # label 중복제거\n",
    "            words[token.lower()]=True # 단어를 소문자로 변경후 사전형식(중복제거)으로 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "26869"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> labelSet에 index 붙여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "labelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2Idx={}\n",
    "for label in labelSet:\n",
    "    label2Idx[label]=len(label2Idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2Idx={'numeric':0,'allLower':1,'allUpper':2,'initialUpper':3,'other':4,'mainly_numeric':5,'contains_digit':6,'PADDING_TOKEN':7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> index의 숫자가 너무 크기 때문에 8차원의 단위 행렬로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseEmbeddings=np.identity(len(case2Idx),dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. gloVe Embedding - 40만개 단어 , 100차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx={}\n",
    "wordEmbeddings=[]\n",
    "fEmbeddings=open(\"./dataset/glove.6B.100d.txt\",encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in fEmbeddings:\n",
    "    split=line.strip().split(\" \") # strip : 공백제거\n",
    "    word=split[0]  # 단어만 추출\n",
    "    \n",
    "    if(len(word2Idx)==0):\n",
    "        word2Idx[\"PADDING_TOKEN\"]=len(word2Idx)\n",
    "        vector=np.zeros(len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"]=len(word2Idx)\n",
    "        vector=np.random.uniform(-0.25,0.25,len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "    \n",
    "    if split[0].lower() in words:\n",
    "        vector=np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[split[0]]=len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEmbeddings=np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Char Features - 단어들의 특징, CNN 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx={\"PADDING\":0,\"UNKNOWN\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c]=len(char2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PADDING - 글자들을 cnn을 위한 padding 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(Sentences):\n",
    "    maxlen=52\n",
    "    \n",
    "    for sentence in Sentences:\n",
    "        char=sentence[2]    # char만 추출\n",
    "        for x in char:\n",
    "            maxlen=max(maxlen,len(x))  # 가장 긴 단어의 길이\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2]=pad_sequences(Sentences[i][2],52,padding='post') # 단어들을 패딩 시켜준다.\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. WordFeatures - 단어의 특징을 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word,caseLookup):\n",
    "    casing='other'\n",
    "    \n",
    "    numDigits=0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits+=1\n",
    "    \n",
    "    digitFraction=numDigits/float(len(word))\n",
    "    \n",
    "    if(word.isdigit()):\n",
    "        casing='numeric'\n",
    "    elif digitFraction>0.5:\n",
    "        casing='mainly_numeric'\n",
    "    elif word.islower():\n",
    "        casing='allLower'\n",
    "    elif word.isupper():\n",
    "        casing='allUpper'\n",
    "    elif word[0].isupper():\n",
    "        casing='initialUpper'\n",
    "    elif numDigits>0:\n",
    "        casing='contains_digit'\n",
    "        \n",
    "    return caseLookup[casing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Embedding 단어와 txt 파일의 단어를 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GloVe에는 40만개의 단어이기 때문에 이거와 실제의 데이터의 문장을 비교하여\n",
    "존재하는 단어들의 vector만 가지고와 저장하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences,word2Idx,label2Idx,case2Idx,char2Idx):\n",
    "    unknownIdx=word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx=word2Idx['PADDING_TOKEN']\n",
    "    \n",
    "    dataset=[]\n",
    "    \n",
    "    wordCount=0\n",
    "    unknownWordCount=0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices=[]\n",
    "        caseIndices=[]\n",
    "        charIndices=[]\n",
    "        labelIndices=[]\n",
    "        \n",
    "        for word,char,label in sentence:\n",
    "            wordCount+=1\n",
    "            if word in word2Idx:\n",
    "                wordIdx=word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx=word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx=unknownIdx\n",
    "                unknownWordCount+=1\n",
    "            charIdx=[]\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "                \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word,case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "        \n",
    "        dataset.append([wordIndices,caseIndices,charIndices,labelIndices])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=padding(createMatrices(trainSentences,word2Idx,label2Idx,case2Idx,char2Idx))\n",
    "dev_set=padding(createMatrices(devSentences,word2Idx,label2Idx,case2Idx,char2Idx))\n",
    "test_set=padding(createMatrices(testSentences,word2Idx,label2Idx,case2Idx,char2Idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label={v:k for k,v in label2Idx.items()}\n",
    "np.save(\"./output/idx2Label.npy\",idx2Label)\n",
    "np.save(\"./output/word2Idx.npy\",word2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 단어들의 길이를 기준으로 1부터 재 정렬하여 리스트에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(data):\n",
    "    l=[]\n",
    "    for i in data:\n",
    "        l.append(len(i[0])) # 단어의 길이 \n",
    "        \n",
    "    l=set(l)    # 중복제거\n",
    "    \n",
    "    batches=[]\n",
    "    batch_len=[]\n",
    "    z=0\n",
    "    \n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0])==i:    # 단어어 개수대로 리스트를 정렬한다.\n",
    "                batches.append(batch)\n",
    "                z+=1\n",
    "        batch_len.append(z)\n",
    "    \n",
    "    return batches,batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch,train_batch_len=createBatches(train_set)\n",
    "dev_batch,dev_batch_len=createBatches(dev_set)\n",
    "test_batch,test_batch_len=createBatches(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 배열로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(dataset,batch_len):\n",
    "    start=0\n",
    "    for i in batch_len:\n",
    "        tokens=[]\n",
    "        caseing=[]\n",
    "        char=[]\n",
    "        labels=[]\n",
    "        data=dataset[start:i]\n",
    "        start=i\n",
    "        for dt in data:\n",
    "            t,c,ch,l=dt\n",
    "            l=np.expand_dims(l,-1)\n",
    "            tokens.append(t)\n",
    "            caseing.append(c)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Deep Learning Model 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TimeDistributed : 3차원 텐서를 입력 받을수 있도록 확장,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> char는 52차원으로 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nchar_input (InputLayer)         (None, None, 52)     0                                            \n__________________________________________________________________________________________________\nchar_embedding (TimeDistributed (None, None, 52, 30) 2820        char_input[0][0]                 \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, None, 52, 30) 0           char_embedding[0][0]             \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, None, 52, 30) 2730        dropout_1[0][0]                  \n__________________________________________________________________________________________________\ntime_distributed_2 (TimeDistrib (None, None, 1, 30)  0           time_distributed_1[0][0]         \n__________________________________________________________________________________________________\nwords_input (InputLayer)        (None, None)         0                                            \n__________________________________________________________________________________________________\ncasing_input (InputLayer)       (None, None)         0                                            \n__________________________________________________________________________________________________\ntime_distributed_3 (TimeDistrib (None, None, 30)     0           time_distributed_2[0][0]         \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, None, 100)    2294900     words_input[0][0]                \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, None, 8)      64          casing_input[0][0]               \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, None, 30)     0           time_distributed_3[0][0]         \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, None, 138)    0           embedding_1[0][0]                \n                                                                 embedding_2[0][0]                \n                                                                 dropout_2[0][0]                  \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, None, 400)    542400      concatenate_1[0][0]              \n__________________________________________________________________________________________________\ntime_distributed_4 (TimeDistrib (None, None, 9)      3609        bidirectional_1[0][0]            \n==================================================================================================\nTotal params: 2,846,523\nTrainable params: 551,559\nNon-trainable params: 2,294,964\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\n",
    "\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "\n",
    "output = concatenate([words, casing,char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output)\n",
    "\n",
    "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0/50\n"
    },
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node time_distributed_1/convolution (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_4804]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-6c96243e87cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_batch_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcasing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcasing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node time_distributed_1/convolution (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_4804]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "\n",
    "    a=Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels,tokens,casing,char=batch\n",
    "        model.train_on_batch([tokens,casing,char],labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(' ')\n",
    "# model.save(\"./output/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation import compute_f1\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Word = {v:k for k,v in word2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    \n",
    "    token_list = []\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "\n",
    "    b = Progbar(len(dataset))\n",
    "\n",
    "    for i,data in enumerate(dataset):\n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        \n",
    "        token_list.append(tokens[0])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "\n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "\n",
    "        b.update(i)\n",
    "\n",
    "    b.update(i+1)\n",
    "\n",
    "    return token_list, predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_print(token, pred, cor):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i,j,k in zip(token, pred, cor):\n",
    "        for x,y,q in zip(i,j,k):\n",
    "            data.append([x,y,q]) # y가 예측 q가 정답\n",
    "    \n",
    "    print(\"{:15}|{:5}|{:5}|{}\".format(\"단어\", \"예측 값\", \"실제 값\",\"정답확인\"))\n",
    "    print(35*'-')\n",
    "    \n",
    "    for i in range(10):\n",
    "        check = ''\n",
    "        random_data = random.choice(data)\n",
    "        \n",
    "        if random_data[1] == random_data[2]: # 1이 예측, 2가 정답\n",
    "            check = 'O'\n",
    "        else:\n",
    "            check = 'X'\n",
    "        \n",
    "        if random_data[0] != 1:\n",
    "            print('{:17}:{:9}{:9}{}'.format(idx2Word[random_data[0]], idx2Label[random_data[1]], idx2Label[random_data[2]], check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1. Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node time_distributed_1/convolution (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_5179]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-2f0768ad0b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredLabels_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrectLabels_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-4779f0d947fb>\u001b[0m in \u001b[0;36mtag_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtoken_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Predict the classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node time_distributed_1/convolution (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_5179]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "token_dev, predLabels_dev, correctLabels_dev = tag_dataset(dev_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'token_dev' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-638c8b83de33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluation_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredLabels_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrectLabels_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'token_dev' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_print(token_dev, predLabels_dev, correctLabels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predLabels_dev' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2e2334cf303c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_f1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredLabels_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrectLabels_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2Label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpre_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predLabels_dev' is not defined"
     ]
    }
   ],
   "source": [
    "pre_dev, rec_dev, f1_dev = compute_f1(predLabels_dev, correctLabels_dev, idx2Label)\n",
    "print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2. Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node time_distributed_1/convolution (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_5179]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-edc3ea3eab60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredLabels_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrectLabels_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-4779f0d947fb>\u001b[0m in \u001b[0;36mtag_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtoken_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Predict the classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\shyun\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node time_distributed_1/convolution (defined at C:\\Users\\ezcare14\\Anaconda3\\envs\\shyun\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_5179]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "token_test, predLabels_test, correctLabels_test = tag_dataset(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'token_test' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b1441bfda509>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluation_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredLabels_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrectLabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'token_test' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_print(token_test, predLabels_test, correctLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predLabels_test' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-1243afcbcc06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_test\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcompute_f1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredLabels_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrectLabels_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2Label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpre_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predLabels_test' is not defined"
     ]
    }
   ],
   "source": [
    "pre_test, rec_test, f1_test= compute_f1(predLabels_test, correctLabels_test, idx2Label)\n",
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shyun",
   "language": "python",
   "name": "shyun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}