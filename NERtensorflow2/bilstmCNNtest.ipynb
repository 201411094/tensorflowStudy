{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##전역 변수\n",
    "vocab_size = 4000\n",
    "max_len = 70\n",
    "tag_size = 10\n",
    "word_len = 25\n",
    "char_size = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일을 읽고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, 'r')\n",
    "    tagged_sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ') # 공백을 기준으로 속성을 구분한다.\n",
    "        splits[-1] = re.sub(r'\\n', '', splits[-1]) # 줄바꿈 표시 \\n을 제거한다.\n",
    "        #word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장한다.\n",
    "        sentence.append([splits[0], splits[-1]]) # 단어와 개체명 태깅만 기록한다.\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences=readfile(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSentences=readfile(\"valid.txt\")\n",
    "testSentences=readfile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperatearray(rawsentence):\n",
    "    sentences, ner_tags = [], [] \n",
    "    for tagged_sentence in rawsentence: # 14,041개의 문장 샘플을 1개씩 불러온다.\n",
    "        sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\n",
    "        sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n",
    "        ner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다.\n",
    "    return sentences, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence, train_tag = seperatearray(trainSentences)\n",
    "valid_sentence, valid_tag = seperatearray(validSentences)\n",
    "test_sentence, test_tag = seperatearray(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n ['Peter', 'Blackburn'],\n ['BRUSSELS', '1996-08-22']]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_sentence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencetochar(sentences):\n",
    "    charpaddedsentence=[]\n",
    "    for sentence in sentences:\n",
    "        newSentence=[]\n",
    "        makesentence =[]\n",
    "        makesentence.extend(sentence)\n",
    "        while len(makesentence)<max_len:        #문장 padding\n",
    "            makesentence.append(\"#\")\n",
    "        if len(makesentence)>max_len:           #문장 MAX 길이에 따라 자름\n",
    "            makesentence=makesentence[:max_len]\n",
    "        for words in makesentence:              \n",
    "            if len(words) > char_size:          #단어 padding\n",
    "                words=words[:char_size]\n",
    "            newSentence.append([words.ljust(char_size,'#')])    #단어 MAX길이에 따라 자름\n",
    "            \n",
    "\n",
    "        charpaddedsentence.append(newSentence)\n",
    "    return charpaddedsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char= sentencetochar(train_sentence)\n",
    "\n",
    "valid_char = sentencetochar(valid_sentence)\n",
    "test_char = sentencetochar(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx={\"#\":0,\"UNKNOWN\":1}\n",
    "for c in \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c]=len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars=[c for c in data[0]]            # Character 분리\n",
    "            Sentences[i][j]=chars # 단어, Chracter, NER을 리스트로\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char = addCharInformation(train_char)\n",
    "valid_char = addCharInformation(valid_char)\n",
    "test_char = addCharInformation(test_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation2(Sentences):\n",
    "    total=[]\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sen=[]\n",
    "        for j,data in enumerate(sentence):\n",
    "            changeInt=[]\n",
    "            for k, chars in enumerate(data):\n",
    "                changeInt.append(char2Idx[chars])\n",
    "                # print(chars)\n",
    "            # print(changeInt)\n",
    "            Sen.append(changeInt) # 단어, Chracter, NER을 리스트로\n",
    "        total.append(Sen)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_char_train = addCharInformation2(train_char)\n",
    "X_char_valid = addCharInformation2(valid_char)\n",
    "X_char_test = addCharInformation2(test_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_train = np.array([np.array(x1) for x1 in X_char_train])\n",
    "X_char_valid = np.array([np.array(x1) for x1 in X_char_valid])\n",
    "X_char_test = np.array([np.array(x1) for x1 in X_char_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4000        #문장 데이터에 있는 모든 단어를 사용하지 않고 높은 빈도수를 가진 상위 약 4,000개의 단어만을 사용\n",
    "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')         #tokenizer 객체 생성\n",
    "src_tokenizer.fit_on_texts(train_sentence)                              #인덱스 구축 \n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras_preprocessing.text.Tokenizer at 0x19c6d389c08>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "tar_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "단어 집합의 크기 : 4000\n개체명 태깅 정보 집합의 크기 : 10\n"
    }
   ],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(train_sentence)\n",
    "y_train = tar_tokenizer.texts_to_sequences(train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22']]\n"
    }
   ],
   "source": [
    "print(train_sentence[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = src_tokenizer.texts_to_sequences(valid_sentence)\n",
    "y_valid = tar_tokenizer.texts_to_sequences(valid_tag)\n",
    "\n",
    "X_test = src_tokenizer.texts_to_sequences(test_sentence)\n",
    "y_test = tar_tokenizer.texts_to_sequences(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[989, 1, 205, 629, 7, 3939, 216, 1, 3], [774, 1872], [726, 150]]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = src_tokenizer.index_word\n",
    "index_to_ner = tar_tokenizer.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'o',\n 2: 'b-loc',\n 3: 'b-per',\n 4: 'b-org',\n 5: 'i-per',\n 6: 'i-org',\n 7: 'b-misc',\n 8: 'i-loc',\n 9: 'i-misc'}"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "index_to_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "기존 문장 : ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n빈도수가 낮은 단어가 OOV 처리된 문장 : ['soccer', '-', 'japan', 'get', 'OOV', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.']\n"
    }
   ],
   "source": [
    "decoded = []\n",
    "for index in X_test[0] : # 첫번째 샘플 안의 인덱스들에 대해서\n",
    "    decoded.append(index_to_word[index]) # 다시 단어로 변환\n",
    "\n",
    "print('기존 문장 : {}'.format(test_sentence[0]))\n",
    "print('빈도수가 낮은 단어가 OOV 처리된 문장 : {}'.format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "# X_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "# y_train의 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자0으로 채움.\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=max_len)\n",
    "y_valid = pad_sequences(y_valid, padding='post', maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "y_test = pad_sequences(y_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "# y_valid = to_categorical(y_valid, num_classes=tag_size)\n",
    "# y_test = to_categorical(y_test, num_classes=tag_size)           ##원핫 인코딩 시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14041, 70)"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_char_train = X_char_train.reshape(14041,70,52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================<< 모델 생성 >>================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = tf.keras.layers.Input(shape=(None, ),dtype='int32', name='modelInput')\n",
    "words = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_input=tf.keras.layers.Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(len(char2Idx),30,embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= tf.keras.layers.Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(52))(conv1d_out)\n",
    "char = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(maxpool_out)\n",
    "char = tf.keras.layers.Dropout(0.5)(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.concatenate([words, char])\n",
    "output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(10, activation='sigmoid'))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nchar_input (InputLayer)         [(None, None, 52)]   0                                            \n__________________________________________________________________________________________________\nchar_embedding (TimeDistributed (None, None, 52, 30) 2790        char_input[0][0]                 \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, None, 52, 30) 0           char_embedding[0][0]             \n__________________________________________________________________________________________________\ntime_distributed_4 (TimeDistrib (None, None, 52, 30) 2730        dropout_2[0][0]                  \n__________________________________________________________________________________________________\ntime_distributed_5 (TimeDistrib (None, None, 1, 30)  0           time_distributed_4[0][0]         \n__________________________________________________________________________________________________\nmodelInput (InputLayer)         [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntime_distributed_6 (TimeDistrib (None, None, 30)     0           time_distributed_5[0][0]         \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, None, 128)    512000      modelInput[0][0]                 \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, None, 30)     0           time_distributed_6[0][0]         \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, None, 158)    0           embedding_2[0][0]                \n                                                                 dropout_3[0][0]                  \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, None, 400)    574400      concatenate_1[0][0]              \n__________________________________________________________________________________________________\ntime_distributed_7 (TimeDistrib (None, None, 10)     4010        bidirectional_1[0][0]            \n==================================================================================================\nTotal params: 1,095,930\nTrainable params: 1,095,930\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[words_input, character_input], outputs=output)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14041 samples, validate on 3250 samples\nEpoch 1/8\n14041/14041 [==============================] - 27s 2ms/sample - loss: 0.1612 - val_loss: 0.1162\nEpoch 2/8\n14041/14041 [==============================] - 21s 2ms/sample - loss: 0.0846 - val_loss: 0.0855\nEpoch 3/8\n14041/14041 [==============================] - 22s 2ms/sample - loss: 0.0561 - val_loss: 0.0563\nEpoch 4/8\n14041/14041 [==============================] - 21s 1ms/sample - loss: 0.0410 - val_loss: 0.0473\nEpoch 5/8\n14041/14041 [==============================] - 21s 1ms/sample - loss: 0.0337 - val_loss: 0.0437\nEpoch 6/8\n14041/14041 [==============================] - 22s 2ms/sample - loss: 0.0299 - val_loss: 0.0386\nEpoch 7/8\n14041/14041 [==============================] - 21s 2ms/sample - loss: 0.0272 - val_loss: 0.0373\nEpoch 8/8\n14041/14041 [==============================] - 21s 2ms/sample - loss: 0.0253 - val_loss: 0.0366\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x19c80ff8648>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "model.fit(x=[X_train, X_char_train],y=y_train,epochs=8,batch_size=128,validation_data=([X_valid, X_char_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n 테스트 정확도: 0.9613\n"
    }
   ],
   "source": [
    "loss_test = model.evaluate([X_test,X_char_test], y_test,verbose=0)\n",
    "accuracy_test = 1-loss_test\n",
    "print(\"\\n 테스트 정확도: %.4f\" % accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN = tf.keras.Model(inputs=character_input, outputs=char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(X_train, y_train, batch_size=128, epochs=8,  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =======================================================================\n",
    "# 모델 3. Subclaissing Model \n",
    "### - many2many bidirectional LSTM with TimeDistributed \n",
    "### - Subclaissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel (tf.keras.Model):\n",
    "    def __init__(self,vocab_size,tag_size):\n",
    "        super().__init__()\n",
    "        self.fEmbedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
    "        self.fBiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), merge_mode='concat')\n",
    "        # self.fBiLSTM_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), merge_mode='concat')\n",
    "        # self.fDense = (tf.keras.layers.Dense(128, activation='softmax'))\n",
    "        self.fTimeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size, activation='softmax'))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fEmbedding(x)\n",
    "        x = self.fBiLSTM(x)\n",
    "        # x = self.fDense(x)\n",
    "        # x = self.fBiLSTM(x)\n",
    "        x = self.fTimeDistributed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSub = TestModel(4000,10)\n",
    "subClassInputs = tf.keras.layers.Input(shape=(70, ),dtype='int32')\n",
    "subClassInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelSub(subClassInputs)\n",
    "modelSub.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "modelSub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelSub.fit(X_train, y_train, batch_size=128, epochs=8,  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (modelSub.evaluate(X_test, y_test,verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================================================\n",
    "# 모델 2. CNN을 추가한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=10 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = np.argmax(y_predicted, axis=-1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
    "true = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
    "    if w != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_ner[t].upper(), index_to_ner[pred].upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595462575451",
   "display_name": "Python 3.7.7 64-bit ('shyun': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}