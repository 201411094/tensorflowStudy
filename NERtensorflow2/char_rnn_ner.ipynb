{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.0.0\n"
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, BatchNormalization\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set()\n",
    "\n",
    "def file2Examples(file_name):\n",
    "  '''\n",
    "  Read data files and return input/output pairs\n",
    "  '''\n",
    "  \n",
    "  examples=[]\n",
    "\n",
    "  with open(file_name,\"r\") as f:\n",
    "\n",
    "    next(f)\n",
    "    next(f)\n",
    "\n",
    "    example = [[],[]]\n",
    "\n",
    "    for line in f:\n",
    "\n",
    "      input_output_split= line.split()\n",
    "\n",
    "      if len(input_output_split)==4:\n",
    "        example[0].append(input_output_split[0])\n",
    "        example[1].append(input_output_split[-1])\n",
    "        labels.add(input_output_split[-1])\n",
    "\n",
    "      elif len(input_output_split)==0:\n",
    "        examples.append(example)\n",
    "        example=[[],[]]\n",
    "      else:\n",
    "        example=[[],[]]\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return examples\n",
    "  \n",
    "# Extract examples from train, validation, and test files which can be found at \n",
    "# https://github.com/davidsbatista/NER-datasets/tree/master/CONLL2003\n",
    "train_examples = file2Examples(\"train.txt\")\n",
    "test_examples = file2Examples(\"test.txt\")\n",
    "valid_examples = file2Examples(\"valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'B-PER', 'B-MISC', 'B-LOC', 'O', 'B-ORG', 'I-MISC', 'I-PER', 'I-LOC', 'I-ORG'}\n{' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '=': 29, '?': 30, '@': 31, 'A': 32, 'B': 33, 'C': 34, 'D': 35, 'E': 36, 'F': 37, 'G': 38, 'H': 39, 'I': 40, 'J': 41, 'K': 42, 'L': 43, 'M': 44, 'N': 45, 'O': 46, 'P': 47, 'Q': 48, 'R': 49, 'S': 50, 'T': 51, 'U': 52, 'V': 53, 'W': 54, 'X': 55, 'Y': 56, 'Z': 57, '[': 58, ']': 59, '`': 60, 'a': 61, 'b': 62, 'c': 63, 'd': 64, 'e': 65, 'f': 66, 'g': 67, 'h': 68, 'i': 69, 'j': 70, 'k': 71, 'l': 72, 'm': 73, 'n': 74, 'o': 75, 'p': 76, 'q': 77, 'r': 78, 's': 79, 't': 80, 'u': 81, 'v': 82, 'w': 83, 'x': 84, 'y': 85, 'z': 86}\n"
    }
   ],
   "source": [
    "# create character vocab\n",
    "all_text = \" \".join([\" \".join(x[0]) for x in train_examples+valid_examples+test_examples])\n",
    "vocab = sorted(set(all_text))\n",
    "    \n",
    "    # create character/id and label/id mapping\n",
    "char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "label2idx = {u:i+1 for i, u in enumerate(labels)}\n",
    "idx2label = np.array(labels)\n",
    "    \n",
    "print(idx2label)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "14986\n3683\n3465\n"
    }
   ],
   "source": [
    "def split_char_labels(eg):\n",
    "      '''\n",
    "      For a given input/output example, break tokens into characters while keeping \n",
    "      the same label.\n",
    "      '''\n",
    "\n",
    "      tokens = eg[0]\n",
    "      labels=eg[1]\n",
    "\n",
    "      input_chars = []\n",
    "      output_char_labels = []\n",
    "\n",
    "      for token,label in zip(tokens,labels):\n",
    "\n",
    "        input_chars.extend([char for char in token])\n",
    "        input_chars.extend(' ')\n",
    "        output_char_labels.extend([label]*len(token))\n",
    "        output_char_labels.extend('O')\n",
    "\n",
    "      return [[char2idx[x] for x in input_chars[:-1]],np.array([label2idx[x] for x in output_char_labels[:-1]])]\n",
    "   \n",
    "train_formatted = [split_char_labels(eg) for eg in train_examples]\n",
    "test_formatted = [split_char_labels(eg) for eg in test_examples]\n",
    "valid_formatted = [split_char_labels(eg) for eg in valid_examples]\n",
    "\n",
    "print(len(train_formatted))\n",
    "print(len(test_formatted))\n",
    "print(len(valid_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[[34 49 40 ...  0  0  0]\n [43 46 45 ...  0  0  0]\n [54 65 79 ...  0  0  0]\n ...\n [ 3  1 36 ...  0  0  0]\n [40 66  1 ...  0  0  0]\n [35 81 78 ...  0  0  0]], shape=(128, 228), dtype=int32)\ntf.Tensor(\n[[4 4 4 ... 0 0 0]\n [3 3 3 ... 0 0 0]\n [2 2 2 ... 0 0 0]\n ...\n [4 4 4 ... 0 0 0]\n [4 4 4 ... 0 0 0]\n [1 1 1 ... 0 0 0]], shape=(128, 228), dtype=int32)\n"
    }
   ],
   "source": [
    "# training generator\n",
    "def gen_train_series():\n",
    "\n",
    "    for eg in train_formatted:\n",
    "        yield eg[0],eg[1]\n",
    "\n",
    "# validation generator\n",
    "def gen_valid_series():\n",
    "\n",
    "    for eg in valid_formatted:\n",
    "        yield eg[0],eg[1]\n",
    "\n",
    "# test generator\n",
    "def gen_test_series():\n",
    "\n",
    "    for eg in test_formatted:\n",
    "        yield eg[0],eg[1]\n",
    "    \n",
    "# create Dataset objects for train, test and validation sets  \n",
    "series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "series_valid = tf.data.Dataset.from_generator(gen_valid_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "series_test = tf.data.Dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE=1000\n",
    "\n",
    "# create padded batch series objects for train, test and validation sets\n",
    "ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
    "ds_series_batch_valid = series_valid.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
    "ds_series_batch_test = series_test.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
    "\n",
    "# print example batches\n",
    "for input_example_batch, target_example_batch in ds_series_batch_valid.take(1):\n",
    "    print(input_example_batch)\n",
    "    print(target_example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "\n",
    "def biLSTM(result):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64), input_shape=(len(result[0]), len(result[0][0]))))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation=\"softmax\"))\n",
    "    model.summary()\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    # history = model.fit(result,\n",
    "    #                     total_label,\n",
    "    #                     epochs=10,\n",
    "    #                     validation_split=0.1,\n",
    "    #                     batch_size=512)\n",
    "    history = model.fit(ds_series_batch, epochs=EPOCHS, validation_data=ds_series_batch_valid,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nbidirectional_1 (Bidirection (None, 128)               57856     \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_4 (Dense)              (None, 32)                4128      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 78,529\nTrainable params: 78,529\nNon-trainable params: 0\n_________________________________________________________________\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'total_label' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-43c3f868fecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbiLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_formatted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-911b56789690>\u001b[0m in \u001b[0;36mbiLSTM\u001b[1;34m(result)\u001b[0m\n\u001b[0;32m     13\u001b[0m                   metrics=['accuracy'])\n\u001b[0;32m     14\u001b[0m     history = model.fit(result,\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0mtotal_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_label' is not defined"
     ]
    }
   ],
   "source": [
    "biLSTM(train_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)+1\n",
    "\n",
    "  # The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "label_size = len(labels)  \n",
    "\n",
    "# build LSTM model\n",
    "def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                            batch_input_shape=[batch_size, None],mask_zero=True),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(label_size)\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab)+1,\n",
    "    label_size=len(labels)+1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# define loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS=20\n",
    "  \n",
    "history = model.fit(ds_series_batch, epochs=EPOCHS, validation_data=ds_series_batch_valid,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "preds = np.array([])\n",
    "y_trues= np.array([])\n",
    "\n",
    "# iterate through test set, make predictions based on trained model\n",
    "for input_example_batch, target_example_batch in ds_series_batch_test:\n",
    "\n",
    "  pred=model.predict(input_example_batch)\n",
    "  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()\n",
    "  y_true=target_example_batch.numpy().flatten()\n",
    "\n",
    "  preds=np.concatenate([preds,pred_max])\n",
    "  y_trues=np.concatenate([y_trues,y_true])\n",
    "\n",
    "# remove padding from evaluation\n",
    "remove_padding = [(p,y) for p,y in zip(preds,y_trues) if y!=0]\n",
    "\n",
    "r_p = [x[0] for x in remove_padding]\n",
    "r_t = [x[1] for x in remove_padding]\n",
    "\n",
    "# print confusion matrix and classification report\n",
    "print(confusion_matrix(r_p,r_t))\n",
    "print(classification_report(r_p,r_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594181842285",
   "display_name": "Python 3.7.7 64-bit ('test': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}